---
title: 并发学习
date: 2021-06-09 22:29:23
tags: 
	- 并发
categories:
	- 并发
---

# CPU线程调度

## 算法概念

假设计算机只有一个 CPU ，则在任意时刻只能执行一条机器指令，每个线程只有获得 CPU 的使用权才能执行指令。 

- 所谓多线程的并发运行，其实是指从宏观上看，各个线程轮流获得 CPU 的使用权，分别执行各自的任务。 
- 在运行池中，会有多个处于就绪状态的线程在等待 CPU ，Java 虚拟机的一项任务就是负责线程的调度，线程调度是指按照特定机制为多个线程分配 CPU 的使用权。 



有两种调度模型：**分时调度模型**和**抢占式调度模型**。 

- 分时调度模型是指让所有的线程轮流获得 CPU 的使用权,并且平均分配每个线程占用的 CPU 的时间片这个也比较好理解。 

- Java 虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用 CPU ，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用 CPU 。处于运行状态的线程会一直运行，直至它不得不放弃 CPU 。



## 线程饥饿概念

饥饿，一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。 

Java 中导致饥饿的原因： 

- 高优先级线程吞噬所有的低优先级线程的 CPU 时间。 
- 线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。 

- 线程在等待一个本身也处于永久等待完成的对象(比如调用这个对象的 wait 方法)，因为其他线程总是被持续地获得唤醒。



## 线程优化级概念

每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实 

现，这个实现是和操作系统相关的(OS dependent)。 

- 我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级 

是一个 int 变量(从1-10)，1 代表最低优先级，10 代表最高优先级。 

- Java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一 

般无需设置线程优先级。



## 进程

### 概念

计算机的核心是CPU，它承担了所有的计算任务，而操作系统是计算机的管理者，它负责任务的调度，资源的分配和管理，统领整个计算机硬件；应用程序是具有某种功能的程序，程序是运行于操作系统之上的。



**进程**是一个具有一定独立功能的程序在一个数据集上的一次动态执行的过程，是操作系统进行资源分配和调度的一个独立单位，是应用程序运行的载体。进程是一种抽象的概念，从来没有统一的标准定义。



进程一般由**程序，数据集合和进程控制块三部分**组成。

- 程序用于**描述进程要完成**的功能，是控制进程执行的**指令集**；
- 数据集合是程序在执行时**所需要的数据和工作区**；

- 程序控制块包含**进程的描述信息**和是进程存在的唯一标志。

进程具有的特征：

**动态性**：进程是程序的一次执行过程，是临时的，有生命期的，是动态产生，动态消亡的；

**并发性**：任何进程都可以同其他进程一起并发执行；

**独立性**：进程是系统进行资源分配和调度的一个独立单位；

**结构性**：进程由程序，数据和进程控制块三部分组成。



## 线程

### 概念

在早期的操作系统中并没有线程的概念，**进程是拥有资源和独立运行的最小单位**，也是程序执行的最小单位。任务调度采用的是时间片轮转的抢占式调度方式，而进程是任务调度的最小单位，每个进程有各自独立的一块内存，使得各个进程之间内存地址相互隔离。

后来随着计算机的发展，对CPU的要求越来越高，进程之间的切换开销较大，已经无法满足越来越复杂的程序的要求了。于是就发明了线程，线程是程序执行中一个单一的顺序控制流程，**是程序执行流的最小单元**，是处理器调度和分派的基本单位。**一个进程可以有一个或多个线程**，**各个线程之间共享程序的内存空间**(也就是所在进程的内存空间)。一个标准的线程由线程ID，当前指令指针PC，寄存器和堆栈组成。

而进程由内存空间(代码，数据，进程空间，打开的文件)和一个或多个线程组成。



### 线程生命状态

NEW--新建

RUNNABLE--运行

BLOCKED--阻塞

WAITTING--等待

TIMED_WAITTING--超时等待

TERMINATED--终结

### 生命周期

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1623554892415-b0bd3eab-2610-42be-b11d-8599e70d0d09.png)

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1623555150026-859acd76-66cc-4ee7-be64-eb5a767a1296.png)

## 协程

### 概念

协程是一种用户级的轻量级线程，一个线程可以有0到多个协程，**协程本质**是一个线程时间分片去执行协程代码段。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。 



同一进程中，在多CPU的环境中，多线程可以并行，即多个线程同时在运行。

同一线程中，多协程只能并发，即同一时间只有一个协程在运行。



优点：

(1) 协程的执行效率非常高。因为子程序切换不是线程切换，而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的性能优势就越明显。

(2)协程不需要多线程的锁机制。在协程中控制共享资源不加锁，只需要判断状态就好了



## 线程模型

### 参考

[线程模型概念](https://blog.csdn.net/gatieme/article/details/51892437)

### 概念

线程是CPU调度的最小单位，可以独立的完成任务；进程是资源拥有的基本单位。

在同一进程中，线程的切换不会引起进程切换。

在不同进程中进行线程切换,如从一个进程内的线程切换到另一个进程中的线程时，会引起进程切换

根据操作系统内核是否对线程可感知，可以把线程分为**内核线程和用户线程**。

而操作系统中想要实现多线程，主要使用3种多线程模型：

● 内核线程模型

● 用户线程模型

● 混合线程模型

###  内核线程模型

Kernel-Level Thread 简称**KLT**，内核线程模型**完全依赖于操作系统内核**，内核保存线程的状态和信息，线程的创建、调度和管理由内核完成，并且系统内核负责将多个线程执行的任务**映射到各个CPU**中去执行。

线程管理的所有工作由内核完成，应用程序没有进行线程管理的代码，只有一个到内核级线程的编程接口. 内核为进程及其内部的每个线程维护上下文信息，调度也是在内核基于线程架构的基础上完成。每个用户线程都直接与一个内核线程相关联



内核线程的优点:

1. 多处理器系统中，内核能够并行执行同一进程内的多个线程
2. 如果进程中的一个线程被阻塞，能够切换同一进程内的其他线程继续执行（用户级线程的一个缺点）

1. 所有能够阻塞线程的调用都以系统调用的形式实现，代价可观



内核线程的缺点：

各种线程的操作都需要在**用户态和内核态之间频繁切换**，消耗太大，速度相对用户线程模型来说要慢。



### 用户线程模型

User-Level Thread 简称ULT，用户线程模型**不依赖操作系统核心**，应用提供创建、同步、调度和管理线程函数来控制用户线程。不需要线程从用户态到内核态切换，速度快。

用户级线程仅存在于用户空间中，此类线程的创建、撤销、线程之间的同步与通信功能，都无须利用系统调用来实现。用户进程利用线程库来控制用户线程(如JAVA调用PThread库)。用户级线程是一种”多对一”的线程映射



优点：线程的各种操作以及切换消耗很低；

缺点：线程的所有操作都需要在用户态实现，线程的调度实现起来异常复杂，并且系统内核对ULT无感知，如果线程阻塞则会引起整个进程的阻塞。



### 组合线程模型

组合线程模型是内核线程和用户线程的混合使用，用户线程仍然是在用户态中创建，用户线程的创建、切换和销毁的消耗很低，用户线程的数量不受限制。而LWP在用户线程和内核线程之间充当桥梁，就可以使用操作系统提供的线程调度和处理器映射功能

# JMM模型

## 概念

**Java内存模型**(Java Memory Model简称JMM)是一种抽象的概念，并不真实存在，它描述的是**一组规则或规范**，通过这组规范定义了程序中各个变量（包括实例字段，静态字段和构成数组对象的元素）的访问方式。**JVM运行程序的实体是线程**，而每个线程创建时JVM都会为其创建一个**工作内存(有些地方称为栈空间)**，用于存储线程私有的数据。

而Java内存模型中规定所有变量都存储在主内存，**主内存是共享内存区域**，所有线程都可以访问，但线程对变量的操作(读取赋值等)必须在工作内存中进行，首先要将变量**从主内存拷贝的自己的工作内存空间**，然后对变量进行操作，操作完成后**再将变量写回主内存**，不能直接操作主内存中的变量，工作内存中存储着主内存中的变量副本拷贝，前面说过，工作内存是每个线程的私有数据区域，因此**不同的线程间无法访问对方的工作内存，线程间的通信(传值)必须通过主内存来完成。**

**概念图：**

![image.png](https://cdn.nlark.com/yuque/0/2021/png/705191/1623595159054-7fed6ad8-a511-4cdf-b53d-9cabd433d336.png)





## JMM与JVM内存的不同

### JMM内存

#### 主内存 

主要存储的是**Java实例对象(****堆内存****)**，所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量还是方法中的本地变量(也称局部变量)，当然也包括了共享的类信息、常量、静态变量。由于是共享数据区域，多条线程对同一个变量进行访问可能会发生线程安全问题。

#### 工作内存

主要存储**当前方法的所有本地变量信息**(工作内存中存储着主内存中的变量副本拷贝)，每个线程**只能访问自己的工作内存**，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，也包括了字节码行号指示器、相关Native方法的信息(**也就是栈帧**)。注意由于工作内存是每个线程的私有数据，线程间无法相互访问工作内存，因此存储在工作内存的数据不存在线程安全问题。(**如上图所示**)



### JMM模型与硬件内存

毋庸置疑的是，多线程的执行最终都是归于硬件处理器去执行的。

对于硬件内存来说只有寄存器、缓存内存、主内存的概念，**并没有工作内存**(线程私有数据区域)和**主内存(堆内存)**之分，也就是说JMM对内存的划分对硬件内存并没有任何影响。

而JMM只是一种抽象的概念和规则，并没有实际存在，**不管是在工作内存还是主内存的数据，在计算机硬件的层次上都是存储在计算机主内存中的**，或者是可能存储在CPU缓存或者寄存器上。

总的来说，JMM和硬件内存是一个相互交叉的关系，是一种抽象概念与真实物理硬件的交叉。

**概念图：**

![image.png](https://cdn.nlark.com/yuque/0/2021/png/705191/1623648032551-b7dd0301-d908-4468-9594-5d1223c732bf.png?x-oss-process=image%2Fresize%2Cw_1500)





## JMM模型的重要性

#### 引言

在JVM中，运行程序的实体是线程，而每个线程创建时，JVM都会为其创建一个工作内存(栈空间)，用于存放现场私有的数据，而现场对于主内存的变量操作必须通过在其工作内存中操作完成。

大概过程是：

- 从主内存中拷贝需要的变量数据到线程的工作内存空间
- 在工作内存中对变量进行操作
- 操作完成后，再将变量写回主内存中(**同步**)

暂引出问题：

如果此时有两个甚至多个线程同时处理主内存中的某个变量，如x=1时，则可能诱发线程安全问题， 该如何解决呢？



## 数据同步的八大原子操作

#### 定义

关于主内存与工作内存之间的交互协议，即一个变量如何从主内存拷贝到工作内存。如何从工作内存同步到主内存中的实现细节。java内存模型定义了8种操作来完成，这8种操作每一种都是原子操作。

（1）lock(锁定)：作用于**主内存**的变量，把一个变量标记为一条线程独占状态 

（2）unlock(解锁)：作用于**主内存**的变量,把一个处于锁定状态的变量释放出来,释放后的变量才能被其他线程锁定 

（3）read(读取)：作用于**主内存**的变量，把一个变量值从主内存传输到线程的工作内存中,以便随后的load动作使用 

（4）load(载入)：作用于**工作内存**的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中 

（5）use(使用)：作用于**工作内存**的变量，把工作内存中的一个变量值传递给执行引擎 

（6）assign(赋值)：作用于**工作内存**的变量，它把一个从执行引擎接收到的值赋给工作内存的变量 

（7）store(存储)：作用于**工作内存**的变量，把工作内存中的一个变量的值传送到主内存中,以便随后的write的操作 

（8）write(写入)：作用于**工作内存**的变量，它把store操作从工作内存中的一个变量的值传送到主内存的变量中 



大致过程：

![image.png](https://cdn.nlark.com/yuque/0/2021/png/705191/1623652732769-eaf9c804-0df7-46a1-922f-c432d69ceb64.png)

#### 规则

1、不允许read和load、store和write操作之一单独出现（即不允许一个变量从主存读取了但是工作内存不接受，或者从工作内存发起会写了但是主存不接受的情况），以上两个操作必须按顺序执行，但没有保证必须连续执行，也就是说，read与load之间、store与write之间是可插入其他指令的。
2、不允许一个线程**丢弃它的最近的assign操作**，即变量在工作内存中改变了之后**必须把该变化同步回主内存**。
3、不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存中。
4、一个**新的变量只能从主内存中“诞生”**，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量，换句话说就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。
5、一个变量在同一个时刻只允许一条线程对其执行lock操作，但lock操作可以被同一个条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。
6、如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值。
7、如果一个变量实现没有被lock操作锁定，则不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定的变量。
8、对一个变量执行unlock操作之前，必须先把此变量同步回主内存（执行store和write操作）。





## 并发三大特性

#### 原子性

定义：即一个操作或者多个操作，要么全部执行并且执行的过程**不会被任何因素打断**，要么就都不执行。

注：Java里，对基本数据类型的变量的读取和赋值操作是原子性操作有点要注意的是，对 于32位系统的来说，long类型数据和double类型数据(对于基本数据类型，byte,short,int,float,boolean,char读写是原子操作)，它们的读写并非原子性的，因为对于32位虚拟机来说，每次原子读写是32位的，而long和double则是64位的存储单元。可能会出现不同现场读取数值不同的情况，如一个线程读的前32位，另一个线程读的后32位。

#### 可见性

定义：当一个线程**修改了某个共享变量**的值，**其他线程是否能够马上得知这个修改的值**。对于串行程序来说，可见性是不存在的，因为我们在任何一个操作中修改了某个变量的值，后续的操作中都能读取这个变量值，并且是修改过的新值。 

**引出**：从JMM模型中可以看出，当多线程操作时可能会出现问题，当某一个线程修改了某个共享变量时，另一个线程也在修改的话，哪一个线程最先修改完，且怎么通知另一个正在修改的线程是一个问题？因为数据同步回主内存是有一定的延迟的。以及指令重排、编译器优化等也可能导致可见性问题。

#### 有序性

定义：指对于单线程的执行代码，我们总是认为代码的执行是按顺序依次执行的，这样的理解并没有毛病，毕竟对于单线程而言确实如此，但对于多线程环境，则可能出现乱序现象，因为程序编译成机器码指令后可能会出现指令重排现象，重排后的指令与原指令的顺序未必一致。

### JMM与三大特性

#### 实现原子性

JVM自身提供的对基本数据那些读写操作可保证原子性。synchronized和Lock(保证任一时刻只有一个线程可访问某一处代码)可实现原子性。

#### 实现可见性

volatile关键字可实现可见性（类似通知功能，通知别的线程）。synchronized和Lock也可保证可见性(原子性基础上，释放锁之前会刷新值到内存中)。

#### 实现有序性

volatile关键字可保证一定的有序性(内存屏障、happens-before)。synchronized和Lock也可保证有序性(线程加锁，相当于顺序执行)



### 指令的重排序

java语言规范规定JVM线程内部维持顺序化语义。**即只要程序的最终结果与它顺序化情况的结果相等**，那么指令的执行顺序可以与代码顺序不一致，此过程叫指令的重排序。

指令重排序的意义是什么？JVM能根据处理器特性（CPU多级缓存系统、多核处理器等）适当的对机器指令进行重排序，使机器指令能更符合CPU的执行特性，最大限度的发挥机器性能以提高运行效率。

 



重排序分三种类型（排序即顺序）：

  1.**编译器优化的重排序**

编译器在不改变单线程程序语义的前提下（代码中不包含synchronized关键字），可以重新安排语句的执行顺序。

  2.**指令级并行的重排序**

现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。

  3.**内存系统的重排序**
由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行



### as-if-serial原则

as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。

为了遵守as-if-serial语义，编译器和处理器不会对**存在数据依赖关系**的操作做重排序，因为这种重排序会**改变执行结果**。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 

如： a=1；b=a ； 先给a赋值为1(a初始为0)，假设重排了变成b=a ；a=1；那b的值不是一致的，则不会重排。



### happens-before原则

cpu的运行极快，而读取主存对于cpu而言有点慢了，在读取主存的过程中cpu一直闲着（也没数据可以运行），这对资源来说造成极大的浪费。所以慢慢的cpu演变成了多级cache结构，cpu在读cache的速度比读内存快了n倍。当线程在执行时，会保存临界资源的副本到私有work memory中，这个memory在cache中，修改这个临界资源会更新work memory但并不一定立刻刷到主存中，那么什么时候应该刷到主存中呢？什么时候和其他副本同步？而且编译器为了提高指令执行效率，是可以对指令重排序的，重排序后指令的执行顺序不一样，有可能线程2读取某个变量时，线程1还未进行写入操作。这就是线程可见性的来源，解决办法就是**happens-before规则**。



#### 定义

- 如果操作1 happens-before 操作2，那么操作1的执行结果将对操作2可见，而且操作1的执行顺序排在操作2之前。
- 两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。



#### 八大原则

- 程序顺序原则：在一个线程内必须保证语义的串行行，即按照代码书写的顺序执行语句。
- 锁定规则：一个解锁(unlock)操作先行发生于后面对同一个锁的加锁(lock)操作。即无论在单线程还是多线程中，**同一个锁**如果处于被锁定状态，那么必须先对锁进行解锁，后面才能继续执行加锁操作。

- volatile规则：对于一个被volatile修饰的变量，写操作先行于读操作。这就可以保证变量的可见性，当volatile变量被线程访问时，都需要从主内存中读取；当某个线程已经修改了volatile变量时，会强行将最新的值刷新到主内存，同时某些线程读取了之前的值会被通知作废，别的线程总是能读取到最新的值。
- 传递规则：操作A先行与操作B，操作B先行与操作C，那么操作A先行与操作C。

- 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作。如果线程A在线程B start之前修改了主内存中的变量C(共享的)，那么线程B start后，也可以看到修改后的变量C值。
- 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生。

- 线程终止规则：线程中所以的操作都先行发生于线程的终止检测，通过Thread.join()方法(等待当前执行的线程终止)、Thread.isAlive()的返回值手段检测到线程已经终止执行。
- 对象终结规则：一个对象的初始化完成先行于它的finalize(()方法的开始。



# volatile

## 作用原理

volatile是Java虚拟机提供的轻量级的同步机制。

两个作用 

- **可见性**：保证被volatile修饰的共享变量对所有线程总数可见的，也就是当一个线程修改了一个被volatile修饰共享变量的值，新值总是可以被其他线程立即得知。 
- **有序性**：禁止指令重排序优化

**但无法保证原子性**。



## 作用实现

### 可见性

```
package com.gx.demo.bingfa;

public class VolatileTest {
    
    private volatile boolean changeFlag = false;

    public void save() {
        this.changeFlag = true;
        System.out.println("线程：" + Thread.currentThread().getName() + " 修改了主存中的共享变量changeFlag");
    }

    public void load() {
        while (!changeFlag) {
        }
        System.out.println("线程：" + Thread.currentThread().getName() + " 感知到了changeFlag变量的修改");
    }

    public static void main(String[] args) {
        VolatileTest sample = new VolatileTest();
        Thread threadA = new Thread(() -> {
            sample.save();
        },"threadA");
        Thread threadB = new Thread(()-> {
                sample.load();
            },"threadB");
        threadB.start();
        try {
            Thread.sleep(2000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        threadA.start();
    }
}
结果：
线程：threadA:修改共享变量changeFlag
线程：threadB 感知到了changeFlag变量的修改
```

### 有序性(禁止指令重排)

禁止指令重排优化指的是：避免多线程环境下程序出现乱序执行的现象。



#### 内存屏障

**概念**：

什么是内存屏障（Memory Barrier）？
内存屏障（memory barrier）是一个CPU指令。

它的作用有两个： 

a) **确保一些特定操作执行的顺序**；

b) **影响一些数据的可见性**(可能是某些指令执行后的结果，保证在内存中可见)。

编译器和CPU可以在保证输出结果一样的情况下对指令重排序，使性能得到优化。插入一个内存屏障，**相当于告诉CPU和编译器先于这个命令的必须先执行**，后于这个命令的必须后执行。内存屏障另一个作用是强制更新一次不同CPU的缓存。

例如，一个写屏障会把这个屏障前写入的数据刷新到缓存，这样任何试图读取该数据的线程将得到最新值，而不用考虑到底是被哪个cpu核心或者哪颗CPU执行的。



#### 硬件层的内存屏障

Intel硬件提供了一系列的内存屏障，主要有：

1. lfence，是一种Load Barrier 读屏障
2. sfence, 是一种Store Barrier 写屏障

1. mfence, 是一种全能型的屏障，具备ifence和sfence的能力
2. Lock前缀，Lock不是一种内存屏障，但是它能完成类似内存屏障的功能。Lock会对CPU总线和高速缓存加锁，可以理解为CPU指令级的一种锁。它后面可以跟ADD, ADC, AND, BTC, BTR, BTS, CMPXCHG, CMPXCH8B, DEC, INC, NEG, NOT, OR, SBB, SUB, XOR, XADD, and XCHG等指令。



#### JVM提供的四类内存屏障

Java内存屏障主要有Load和Store两类。 
对Load Barrier(**读**)来说，在读指令前插入读屏障，可以让高速缓存中的数据失效，重新从主内存加载数据 
对Store Barrier(**写**)来说，在写指令之后插入写屏障，能让写入缓存的最新数据写回到主内存



对于Load和Store，在实际使用中，又分为以下四种：

| 屏障类型   | 指令例子                   | 说明用途                                                     |
| ---------- | -------------------------- | ------------------------------------------------------------ |
| LoadLoad   | Load1,Loadload,Load2       | 确保Load1所要读入的数据能够在被Load2和**后续的load**指令访问前读入 |
| StoreStore | Store1，StoreStore，Store2 | 确保Store1的数据在Store2以及**后续Store指令**操作相关数据之前对其它处理器可见（例如向主存刷新数据） |
| LoadStore  | Load1; LoadStore; Store2   | 确保Load1的数据在Store2和**后续Store**指令被刷新之前读取。在等待Store指令可以越过loads指令的乱序处理器上需要使用LoadStore屏障。 |
| StoreLoad  | Store1; StoreLoad; Load2   | 确保Store1的数据在被Load2和**后续的Load**指令读取之前对其他处理器可见 |

#### volatile的有序性实现

JMM针对编译器制定的volatile重排序规则表。 

第一个操作  第二个操作：普通读写 	第二个操作：volatile读 					 第二个操作：volatile写 

普通读写 	 可以重排 							可以重排 												不可以重排 

volatile读    不可以重排 						不可以重排 											不可以重排 

volatile写    可以重排 							不可以重排				        					  不可以重排



### 原子性

volatile无法保证原子性。

```
static volatile int i = 0;
public static void caculate(){
	i++;
}
```

i++分为：先去读取i的值，然后再+1写入一个新的值，两个步骤完成，本身不具备原子性。

假如在第一步完成之后，第二步执行之前时，有线程在此时读取了i在内存中的值，那么这个线程会和开始那个线程相当于要对i执行一样的操作，i结果都是1。也就造成线程安全失败了。

解决办法：对执行方法添加synchronized，但是synchronized一样具备了可见性，可以不用volatile修饰了。



# CPU缓存一致性协议(MESI)

## 计算机的缓存一致性

计算机在运行程序时，每条指令都是在CPU中执行的，在执行过程中势必会涉及到数据的读写。我们知道程序运行的数据是存储在主存中，这时就会有一个问题，读写主存中的数据没有CPU中执行指令的速度快，如果任何的交互都需要与主存打交道则会大大影响效率，所以就有了**CPU高速缓存(Cache Memory)**。CPU高速缓存为某个CPU独有，只与在该CPU运行的线程有关。
	有了CPU高速缓存虽然解决了效率问题，但是它会带来一个新的问题：**数据一致性**。**在程序运行中，会将运行所需要的数据复制一份到CPU高速缓存中，在进行运算时CPU不再与主存打交道，而是直接从高速缓存中读写数据，只有当运行结束后才会将数据刷新到主存中。**
解决缓存一致性方案有两种：

1. 通过在总线加LOCK#锁的方式
2. 通过缓存一致性协议



## CPU高速缓存(Cache Memory)

### 存在的意义

CPU高速缓存是为了解决CPU速率和主存访问速率差距过大问题。

- CPU：根据摩尔定律，CPU会以每18个月的时间将访问速度翻一番，相当于每年增长60%。
- 内存：内存的访问速度虽然也在不断增长，却远没有这么快，每年只增长 7% 左右

到今天来看，一次内存的访问，大约需要 120 个 CPU Cycle，这也意味着，在今天，CPU 和内存的访问速度已经有了 120 倍的差距。



因此引入了“高速缓存”，CPU厂商在CPU中内置了少量的高速缓存以解决I\O速度和CPU运算速度之间的不匹配问题。（高速缓存是插在CPU寄存器和主存之间的缓存存储器）

- **高速缓存**(CPU Cache)：用于平衡 CPU 和内存的性能差异，分为 L1/L2/L3 Cache。其中 L1/L2 是 CPU 私有，L3 是所有 CPU 共享。
- **缓存行**(Cache Line)：**高速缓存的最小单元**，一次从内存中读取的数据大小。常用的 Intel 服务器 Cache Line 的大小通常是 64 字节。



### 存储器层次结构

存储器在计算机内是有层次，就像一个金字塔，塔顶的存储器速度极高，但容量很小，越往下，速度越慢，但容量越大。

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625816720465-cc5c5430-11e5-4470-ab26-e95304ef949e.png)

### 缓存如何提高效率

计算机程序运行**遵循局部性原则**。局部性原理是指程序在执行时呈现出局部性规律，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。



局部性原理又表现为：时间局部性和空间局部性。

- 时间局部性(Temporal Locality)：指如果某条指令一旦被执行，很有可能不久后还会再次被执行；如果某个数据一旦被访问了，很有可能不久之后还会再次被访问。如：循环、递归等。
- 空间局部性(Spatial Locality)：指如果某个存储单元一旦被访问了，很有可能不久后它附件的存储单元也会被访问。如连续创建多个对象、数组等。



具有良好局部性的程序比差的程序更多的倾向于从存储器层次结构较高层次处访问数据，因此运行的更快，尤其是执行大数据量的算术运算。



### 单核下高速缓存的CPU执行流程

- 1.程序和数据都被加载到主内存中
- 2.执行指令和数据被加载到CPU的高速缓存中，进行逻辑处理

- 3.CPU执行指令再将处理后的结果写到CPU的高速缓存中
- 4.CPU的高速缓存再将数据写回(更新)到主内存中

列举缓存结构图：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625818892776-efd1b427-253c-40b0-90d5-6e9b9d3ce020.png)



### 多级缓存结构

高速缓存是插在CPU寄存器和主存之间的缓存存储器，称为**L1高速缓存**，基本是由SRAM（static RAM）构成，访问时大约需要4个始终周期。刚开始只有L1高速缓存，后来CPU和主存访问速度差距不断增大，**在L1和主存之间增加了L2高速缓存**，可以在10个时钟周期内访问到。现代CPU又**增加了一个更大的L3高速缓存**，可以在大约50个时钟周期内访问到它。

列举多级缓存结构图：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625818559315-8b2c8c3e-ac88-40aa-a1bd-2e8c37dcd20f.png)



## 多核CPU多级缓存下的MESI

### MESI的缓存状态

CPU中每个缓存行（Caceh line)使用4种状态进行标记，使用2bit来表示:

| 状态                     | 描述                                                         | 监听任务                                                     | 状态转换                                                     |
| ------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| M 修改 (Modified)        | 该Cache line有效，数据被修改了，和内存中的数据不一致，数据只存在于本Cache中。 | 缓存行必须时刻监听所有试图读该缓存行相对就主存的操作，这种操作必须在缓存将该缓存行写回主存并将状态变成S（共享）状态之前被延迟执行。 | 当被写回主存之后，该缓存行的状态会变成独享（exclusive)状态。 |
| E 独享、互斥 (Exclusive) | 该Cache line有效，数据和内存中的数据一致，数据只存在于本Cache中。 | 缓存行也必须监听其它缓存读主存中该缓存行的操作，一旦有这种操作，该缓存行需要变成S（共享）状态。 | 当CPU修改该缓存行中内容时，该状态可以变成Modified状态        |
| S 共享 (Shared)          | 该Cache line有效，数据和内存中的数据一致，数据存在于很多Cache中。 | 缓存行也必须监听其它缓存使该缓存行无效或者独享该缓存行的请求，并将该缓存行变成无效（Invalid）。 | 当有一个CPU修改该缓存行时，其它CPU中该缓存行可以被作废（变成无效状态 Invalid）。 |
| I 无效 (Invalid)         | 该Cache line无效。                                           | 无                                                           | 无                                                           |

注意： 对于M和E状态而言总是精确的，他们在和该缓存行的真正状态是一致的，而S状态可能是非一致的。



### MESI状态间的转换

**MESI状态转换图：**

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625842463628-2ca57a37-9bed-4e6c-a682-7913f01ff284.png)

- 本地读取(Local Read): 本地cache读取本地cache中的数据
- 远端读取(Remote Read): 其它cache读取本地cache中的数据

- 本地写入(Local Write): 本地cache将数据写入本地cache中
- 远端写入(Remote Write): 其它cache将数据写入本地cache中



**装换说明**：

第一：

某个CPU（CPU A）发起**本地写请求**（Local Write），比如对某个内存地址的变量赋值，如果此时所有CPU的Cache中**都没加载**此内存地址，**即此内存地址对应的Cache Line为无效状态**（Invalid），则CPU A中的Cache Line保存了最新内存变量值以后，其状态被修改为**Modified**。

随后，如果CPU B发起对同一个变量的**读操作**（Remote Read），则CPU A在总线上**嗅探到这个读请求**以后，先将**Cache Line里修改过的数据回写**（Write Back）到Memory中，然后在内存总线上**放一份Cache Line的拷贝**作为应答，最后再将**自身的Cache Line的状态修改为Shared**，由此产生的结果是CPU A与CPU B里对应的Cache Line的状态都为Shared。



第二：

在第一点的基础上，CPU A发起本地写请求导致自身的Cache Line状态变为**Modified**以后，如果此时CPU B发起**同一个内存地址的写请求**（Remote Write），则我们看到状态图里此时CPU A的Cache Line状态为**Invalid**。				

其原因是如下：CPU B此时发出的是一个特殊的请求——“**读并且打算修改数据**”（read with intent to modify），当CPU A从总线上**嗅探**到这个请求后，会**先阻止此请求并取得总线的控制权**（Takes control of bus），随后将Cache Line里**修改过的数据回写**（Write Back）到Memory中，再将此Cache Line的状态修改为**Invalid**（这是因为其他CPU要改数据，所以**没必要改为Shared**了）。

与此同时，CPU B发现**之前的请求并没有得到响应**，于是**重新**再发起一次请求，此时由于所有CPU的Cache里都没有内存副本了，所以CPU B的Cache就从Memory中加载最新的数据到Cache Line中，随后修改数据，然后改变Cache Line的状态为**Modified**。



下图表示了当一个缓存行(Cache line)的调整的状态的时候，另外一个缓存行(Cache line)需要调整的状态。

| 状态  | M    | E    | S    | **I** |
| ----- | ---- | ---- | ---- | ----- |
| **M** | ×    | ×    | ×    | √     |
| **E** | ×    | ×    | ×    | √     |
| **S** | ×    | ×    | √    | √     |
| **I** | √    | √    | √    | √     |

```
举例：
比如有某个变量a=1；
1.cache line处于M(修改)状态，其它cache对此变量都应是I(无效)状态
2.cache line处于S(共享)状态，其它cache对此变量可以是I(无效)状态，也可以是S(共享)状态
```



### 多核缓存示意图

比如有多个线程(此处3个)，共同读取主存中的某个变量int z=1；

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625844949818-4f0d94fb-c5f5-4481-8bb4-82a2ba6fc163.png)



### 单核下的数据读取

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625845092990-b899e68d-fa7f-449f-967c-cacf0b75a467.png)

1.CPU A发出了一条读取数据的指令，需要从主存中读取变量z。

2.首先从主存中将数据读取到BUS总线中。

3.再通过BUS总线读取到CPU A的缓存中。也就是Remote Read，此时cache line的状态需修改为E(独享)。



### 多核下的数据读取

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625845445601-60e4fec8-f333-4d45-ad5e-adb9b2c80216.png)

1.CPU A发出了一条读取数据的指令，需要从主存中读取变量z。

2.首先从主存中将数据读取到BUS总线中。

3.再通过BUS总线读取到CPU A的缓存中。也就是Remote Read，此时cache line的状态需修改为E(独享)。

4.CPU B也发出了一条读取数据的指令，需要从主存中读取变量z。

5.CPU B尝试从主存中读取变量z，但被CPU A嗅探到了有地址冲突。此时CPU A对数据做出状态更改，为S(共享)，**根据上面表格得到其它cache line的此变量需要是S或者I**，于此当变量被读取到CPU B时也是S状态。

### 单核下的数据修改

1.CPU A发出了一条修改数据的指令，需要从主存中修改变量z。(一开始没其它cache读取，状态为I)

2.首先从主存中将数据读取到BUS总线中。

3.再通过BUS总线读取到CPU A的缓存中，进行Local write，此时cache line的状态需修改为M(修改)。

4.修改完了，再将数据回写到主存中。

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625897560180-3e7ae838-f57e-4ce4-b8aa-0ae6a8f19889.png)

### 多核下的数据修改及数据同步

**修改**：(承接上面多核读取结束后，CPU A对数据进行了修改)

1.CPU A进行Local write，修改变量z=2，此时要将其cache line的状态修改为M(修改)，并通知有缓存了z变量的CPU，此处即CPU B。

2.CPU B需要将本地cache 中的z设置为I(无效)

3.CPU A对变量z进行赋值

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625898091695-c8f9e690-11ec-4b37-8d8a-b884916ef65a.png)

**同步**：(涉及两种情况：其它CPU，如CPU B此时要读取z，或者CPU B此时要读取并修改z)

**CPU B此时要读取z**

1.CPU B发出读取z的指令（Remote read）

2.CPU A在总线上**嗅探到这个读请求**以后，先将**Cache Line里修改过的数据回写**（Write Back）到Memory中，然后在内存总线上**放一份Cache Line的拷贝**作为应答。

3.将**自身的Cache Line的状态修改为Shared**，由此产生的结果是CPU A与CPU B里对应的Cache Line的状态都为Shared。

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625899258376-d7799e03-8233-4613-a90a-62b517f164ef.png)

**CPU B此时要读取并修改z**

1.CPU B发出读取z的指令（Remote Write）

2.CPU A在总线上**嗅探到这个读请求**以后，先阻止CPU B修改，然后将**Cache Line里修改过的数据回写**（Write Back）到Memory中，直接将自身cache line设置为I(无效)状态

3.CPU B再次获取修改请求，此时变量z在其它cache中没有缓存副本了，CPU B直接从主存中拿到最新的数据，进行修改操作，状态设置为M(修改)。

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625899810303-2478ad77-6ae0-4a8c-a2ec-86f76941496b.png)



## MESI问题及优化

### 伪共享(False Sharing)

#### 问题定义

说回CPU缓存，**缓存行（cache line）**是CPU缓存的**基本单位**，缓存行通常是 32/64 字节,前面说了局部性原理。

当我们访问一个数据时，获取一个值后，其相邻的值也被缓存到就近的缓存行中。比如访问一个long类型数组，当数组中的一个值被加载到缓存中，它会额外加载另外7个，以致你能非常快地遍历这个数组。因此可以非常快速的遍历在**连续的内存块**中分配的任意数据结构。

但是没有任何是完美的存在，比如：当有多个线程操作不同的成员变量，但正好这多个变量处于相同的缓存行。如图：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625900702185-4a4c9878-86eb-4909-b87f-7e8b0111158d.png)

```
注释：一个运行在处理器core1上的线程想要更新变量 X 的值，同时另外一个运行在处理器core2上的线程想要更新变量 Y 的值。
但是，这两个频繁改动的变量都处于同一条缓存行。两个线程就会轮番发送 RFO (Request For Owner) 消息，占得此缓存行的拥有权。
当 core1 取得了拥有权开始更新 X，则 core2 对应的缓存行需要设为 I 状态(失效态)。
当 core2 取得了拥有权开始更新 Y，则core1对应的缓存行需要设为 I 状态(失效态)。
轮番夺取拥有权不但带来大量的 RFO 消息，而且如果某个线程需要读此行数据时，L1 和 L2 缓存上都是失效数据，只有L3缓存上是同步好的数据。从前面的内容我们知道，读L3的数据会影响性能，更坏的情况是跨槽读取，L3 都出现缓存未命中，只能从主存上加载。
```

#### 问题解决

1.padding
防止其他数据导致伪共享的问题常用增加**padding**，叫做**缓存行填充**的方式来解决，例如在前后加上无用的数据。

2.注解

在JDK1.8中，新增了一种注解**@sun.misc.Contended**，来使各个变量在Cache line中分隔开。

注意，jvm需要添加参数**-XX:-RestrictContended**才能开启此功能 。类前加上代表整个类的每个变量都会在单独的cache line中。属性前加代表该属性会在单独的cacheline中。



### CPU切换状态堵塞

#### 问题定义

众所周知，CPU的处理数据是非常快的，但MESI下，涉及到各个不同cache之间状态的转换通知(消息传递)，这会耽误大量的时间(处理延迟)。而且CPU会一直等待消息传递和回应完成，其中的时间远大于一个指令的执行时间。

比如：CPU A需进行变量z的修改(Local Write)，那必须通知其它CPU需要对缓存了z的缓存行置为I(无效)状态，并且要等所有CPU都响应确认。这等待期间会堵塞处理器，降低其性能等。



#### 问题解决

**存储缓存**(Store Buffere)

为了解决等待太长时间避免资源浪费等，引入了store buffere。

处理器将想要写回到主存的数据写入到store buffere中，然后继续处理自己的事情。当发出去的所有设置无效状态的通知都响应了后，数据才会最终被同步到主存中去。

风险一：处理器会从store buffere中尝试加载数据，但其还没提交。称为store forwading，即当加载的时候，如果store buffere中有数据就进行返回；如果没有才能读取自己缓存里面的数据。

风险二：store buffere中的缓存什么时候能同步到主存中，没有任何保证。



**内存屏障**(Nenory Barriers)

- 写屏障 Store Memory Barrier是一条告诉处理器在执行这之后的指令之前，应用所有已经在存储缓存（store buffer）中的保存的指令。
- 读屏障Load Memory Barrier是一条告诉处理器在执行任何的加载前，先应用所有已经在失效队列中的失效操作的指令。

在相关代码前使用对应的读写屏障，保证数据的一致性。

# 同步器

## 概念及意义

在多进程(线程)中，多个进程(线程)读取共享资源时会存在**竞争条件。**计算机中通过设计**同步器**来协调进程(线程)之间执行顺序。**同步器**作用就类似一个安检人员，可以协调旅客按顺序通过。

在Java中，**同步器**可以理解为一个对象，它根据自身状态协调线程的执行顺序。

比如锁（Lock），信号量（Semaphore），屏障（CyclicBarrier），阻塞队列（Blocking Queue）。

这些同步器在功能设计上有所不同，但是内部实现上有共通的地方。

（同步器的设计一般包含几个方面：**状态变量设计（同步器内部状态）**，**访问条件设定**，**状态更新**，**等待方式**，**通知策略**。）



在多线程编程中呢，也一样会可能出现多个线程同时访问同一个共享、可变资源的情况，这种资源称之为**临界资源**。比如文件、变量、对象等。

但线程执行的这个过程是不可控制的，需要同步器，即同步机制来处理，以保证线程的安全访问。



## 解决办法

了解了同步器存在的意义，基本可以知道并发模式的安全问题基本都是用此解决的，即**采用序列化方式访问临界资源**。也就是说，保证在同一时刻下，只有一个线程可以访问临界资源，实现**同步的互斥**。



在Java里，提供了两种方式实现：Lock锁和Synchronized(**因为同步器的本质就是加锁实现**)



# synchronized

## 发展

- JDK1.6之前：

synchronized 是一个**重量级锁**，主要通过内部对象Monitor实现(反编译字节码可提现)，而Monitor锁又是依赖于底层操作系统的Mutex Lock(互斥锁，互斥量)，调用Pthread库实现的，而Pthread库是处于系统的内核空间中，JVM存在于用户空间中，**故此非常耗时**。

- JDK1.6之前出现问题后：

**AQS**(AbstractQueuedSynchronizer)出现了，由国外某大佬用JAVA实现的各种锁，**保证了锁的可重入性和公平性，最重要的是耗时问题的解决**。

- JDK包括1.6之后：

在Java被oracle收购以后，为了实现自己的并发框架，对synchronized做了升级和改良，引入了**偏向锁、轻量级锁**等，让锁有了升级机制，解决了耗时和公平性等问题。



## 加锁方式

- 对于普通同步方法，锁是当前实例对象。
- 对于静态同步方法，锁是当前类的Class对象。

- 对于同步方法块，锁是synchronized括号里配置的对象。



## synchronized原理

对象头、各个锁hashcode 的存放位置

synchronized是一种**对象锁**(锁的是对象而非引用)，锁的**粒度是对象**，用来实现临界资源的同步于互斥，也具有**可重入性**。

JVM是基于进入和退出Moniter对象来实现方法同步和代码块同步，但两者细节有所区别。代码块同步时使用monitorenter和monitorexit指令实现的，方法是通过添加标志ACC_SYNCHRONIZED实现的。

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1624283098595-d27364f4-10b5-46b6-8d55-73625b5e6014.png)

而synchronized正是基于JVM的**内置锁Monitor(监视器锁)**实现的。监视器锁的实现以来底层操作系统的Mutex Lock(互斥锁)实现，它是一个重量级锁，性能较低。



## synchronized实现

**Java对象头和monitor是实现synchronized的基础**。



### Java对象头

synchronized用的锁是存在Java对象头里的。

在HotSpot，对象在内存中存储的布局可以分为3个区域：

1对象头（Header）

2实例数据（Instance Date）

3对齐填充（Padding）

**对象头**由Mark Word(标记字段)和Klass pointer(类型指针)组成。

Mark Word：(32位占4字节，64位占8字节)用于存储对象自身的运行时数据，如哈希码(Hash Code)、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。(**对象和数组稍有差别，数组长度占4字节**)。

Klass pointer：(开启指针压缩占4字节，不开启占8字节)指对象指向它的类元数据的指针，JVM通过这个指针来确定这个对象是哪个类的实例。

下图是内存布局大概分布：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1617776742667-43bdf030-6f55-4d31-917b-2351afc45884.png)

下图是32位虚拟机对象头各锁对应的信息；

对象头一般占有两个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit）），它是实现轻量级锁和

偏向锁的关键。

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1617776860916-57982473-5348-4a77-a956-e02d2d836f0a.png)



### Monitor

什么是Monitor？我们可以把它理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。 

与一切皆对象一样，所有的Java对象是天生的Monitor，每一个Java对象都有成为Monitor的潜质，因为在Java 

的设计中 ，每一个Java对象自打娘胎里出来就带了一把看不见的锁，它叫做**内部锁或者Monitor锁**。 



Monitor 是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列 

表。每一个被锁住的对象都会和一个monitor关联（对象头的MarkWord中的LockWord指向monitor的起始地 

址），同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。

如：某个线程进入了monitor(初始为0)，进入数+1，重复进入+1，退出-1。为0代表没有线程进入。

其结构如下：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1624284866274-1f18d8a6-477e-48de-af8a-1c1e9bb4fc35.png)

- Owner：初始时为NULL表示当前没有任何线程拥有该monitor record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为NULL； 
- EntryQ:关联一个系统互斥锁（semaphore），阻塞所有试图锁住monitor record失败的线程。 

- RcThis:表示blocked或waiting在该monitor record上的所有线程的个数。 
- Nest:用来实现重入锁的计数。 

- HashCode:保存从对象头拷贝过来的HashCode值（可能还包含GC age）。 
- Candidate:用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。Candidate只有两种可能的值：0表示没有需要唤醒的线程，1表示要唤醒一个继任线程来竞争锁。



## 锁膨胀升级过程

锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。

随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁，但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级。

从JDK 1.6 中**默认是开启**偏向锁和轻量级锁的，可以通过-XX:-UseBiasedLocking来禁用偏向锁。



### 偏向锁

HotSpot作者经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了**减少同一线程获取锁**(会涉及到一些CAS操作,耗时)的代价而引入偏向锁。

偏向锁的核心思想是：如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，**当这个线程再次请求锁时**，获取锁无需再做任何同步操作，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。

但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，**偏向锁失败后，并不会立即膨胀为重量级锁**，而是先升级为轻量级锁。



### 轻量级锁

**偏向锁失败后，并不会立即膨胀为重量级锁**，而是先升级为轻量级锁。与之同时，Mark Word中锁结构也会变成轻量级锁的结构。适用场景：绝大部分的锁，在整个同步周期内都不存在竞争，即不太存在同一时间访问同一锁的场合，不然会导致膨胀为重量级锁。



### 自旋锁

轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。（JVM让锁自旋，就是做空循环，一般50-100看具体设置）如果到了时间还没获取锁，就会在操作系统层面挂起线程，膨胀升级为重量级锁。



### 适应性自旋锁 

JDK 1.6引入了更加聪明的自旋锁，即自适应自旋锁。

所谓自适应就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。它怎么做呢？线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。

反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。



### 锁消除 

为了保证数据的完整性，我们在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，JVM检 

测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。**锁消除的依据是逃逸分析的数据支** 

**持**。 

如果不存在竞争，为什么还需要加锁呢？所以锁消除可以节省毫无意义的请求锁的时间。变量是否逃逸， 

对于虚拟机来说需要使用数据流分析来确定，但是对于我们程序员来说这还不清楚么？我们会在明明知道 

不存在数据竞争的代码块前加上同步吗？但是有时候程序并不是我们所想的那样？我们虽然没有显示使用 

锁，但是我们在使用一些JDK的内置API时，如StringBuffffer、Vector、HashTable等，这个时候会存在隐 

形的加锁操作。比如StringBuffffer的append()方法，Vector的add()方法。（如果JVM检测到变量没有逃 

逸，则会将其锁消除掉） 



**锁消除**：前提是java必须运行在server模式（server模式会比client模式作更多的优化），**同时必须开启逃逸分析** 

-XX:+DoEscapeAnalysis 开启逃逸分析 

-XX:+EliminateLocks 表示开启锁消除。 



**逃逸分析** 

使用逃逸分析，编译器可以对代码做如下优化： 

一、同步省略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 

二、将堆分配转化为栈分配。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。

三、分离对象或标量替换。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存， 

而是存储在CPU寄存器中。 



### 锁粗化 

在使用同步锁的时候，需要让同步块的作用范围尽可能小—仅在共享数据的实际作用域中才进行同步，这 

样做的目的是为了使需要同步的操作数量尽可能缩小，如果存在锁竞争，那么等待锁的线程也能尽快拿到 

锁。 

**锁粗化概念**：就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。如：vector每次 

add的时候都需要加锁操作，JVM检测到对同一个对象（vector）连续加锁、解锁操作，会合并一个更大范 

围的加锁、解锁操作，即加锁解锁操作会移到for循环之外。

再如一个类有好几个synchronized(xxObj)方法，都是锁的同一个对象，编译器会将其代码块操作都放到一个锁下面。

### 升级过程图

参考：https://gorden5566.com/post/1019.html

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1624344028666-a76d5b2c-136c-4f49-a3c3-5d671185de07.png)



# Lock接口

锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源（但是有些锁可以允许多个线程并发的访问共享资源，比如读写锁）。在Lock接口出现之前，Java程序依靠synchronized关键字实现锁功能，而Java5之后，并发包(java.concurrent.util)新增了Lock接口(及相关实现类)来实现锁功能。它提供了与synchronized类似的同步功能，只是是**显示的获取和释放锁**（而synchronized是**隐式的获取和释放锁）**。也拥有了更多的synchronized不具备的同步特性，如锁的获取与释放可以多个操作、可中断的获取锁、超时的获取锁等。

synchronized是**隐式的获取和释放锁**，简化了同步的管理，但扩展性比较差，因为它固化了锁的获取和释放，必须先获取再释放。

如：

```
Lock lock = new ReentrantLock();
lock.lock();
try{

}finally{
  lock.unlock();
}
```

| 特性               | 描述                                                         |
| ------------------ | ------------------------------------------------------------ |
| 尝试非阻塞地获取锁 | 当现场尝试获取锁时，如果这一时刻锁没有其他线程占有，则成功获取并持有锁 |
| 能被中断地获取锁   | 与synchronized不同的是，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常会被抛出，且同时会释放锁lock.lockInterruptibly() |
| 能超时获取锁       | 在指定的截止时间之前获取锁，如果截止时间到了仍然无法获取锁，则返回falselock.tryLock(timeout,TimeUnit) |



# 队列同步器AQS

参考：

https://www.cnblogs.com/micrari/p/6937995.html

[https://ifeve.com/java%e5%b9%b6%e5%8f%91%e4%b9%8baqs%e8%af%a6%e8%a7%a3/](https://ifeve.com/java并发之aqs详解/)



## 简介

队列同步器AbstractQueuedSynchronizer，是用来**构建锁或者其他同步组件的基础框架**，它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作，并发包(java.concurrent.util)的作者Doug Lea。比如等待队列、条件队列、锁的独占和共享等，都是基于AQS，其定义了一套多线程访问共享资源的同步器框架。



## AQS的设计理念

前面提到过：同步器的设计一般包含几个方面：**状态变量设计（同步器内部状态）**，**访问条件设定**，**状态更新**，**等待方式**，**通知策略**。用Java实现的队列同步器AQS也是如此。

同步器的设计是**基于模版方法模式的**，也就是说，使用者需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模版方法，而这些模版方法将会调用使用者重写的方法。

与synchronized对比而言，就Lock接口多了一些特性，如锁的获取与释放可以多个操作、可中断的获取锁、超时的获取锁等。AQS也包括这些特性，以及实现了锁的公平性、非公平性，当然还有可重入性。

```
大概猜想：
Lock lock = new 锁；
lock.lock();//加锁
for(;;){
	if(CAS操作){//加锁成功就跳出
    	break；
    }
    //否则需要等待，用什么数据结构保存？
    //即公平性与非公平性，公平即保证进来的顺序排序，非公平即不保证顺序，
    //总结只能用队列
    queue.put(thread);//存当前线程信息
    //然后阻塞
    LockSupport.park(thread);
}
//TODO处理业务逻辑

lock.unlock();//解锁
queue.pop();//移出队列，即下一线程为队首
LockSupport.unpark(thread);//唤醒下一线程
```



## ReentrantLock



ReentrantLock

```
ReentrantLock lock = new ReentrantLock();
lock.lock();
try{

}finally{
  lock.unlock();
}
```



## AQS的接口框架



AQS它内部维护**volatile int state**（代表共享资源的可用状态）和一个**FIFO线程等待队列**（多线程争用资源被阻塞时会进入此队列）。

state的三种访问方式：

- getState()：返回当前的同步状态值
- setState(int)：设置当前的同步状态值

- compareAndSetState(int expect, int update)：原子操作替换状态值



AQS定义了两种队列

- 同步等待队列(CLH队列)
- 条件等待队列



AQS定义两种资源共享方式：

- Exclusive（独占，只有一个线程能执行，如ReentrantLock）
- Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。



不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法：

- isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。
- tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。

- tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。
- tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。

- tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。



## AQS的实现与部分源码理解

从实现角度分析同步器是如何完成线程同步的，主要包括：同步队列、独占式锁的获取与释放、共享式锁的获取与释放等同步器的核心数据结构与模版方法。



### 同步队列

同步器依赖内部的同步队列(**一个FIFO双向队列**)又名**CLH队列**(由Craig、Landin、Hagersten三人发行的基于双向链表的队列)来完成同步状态的管理。

当前线程获取同步状态失败时，同步器就会将当前线程以及等待状态等信息**构造成一个节点(Node)**并将其加入同步队列，同时会**堵塞**当前线程，当同步状态**释放**时，会把**首节点**中的线程**唤醒**，使其再次尝试获取同步状态。



Node的属性说明

| 属性            | 说明描述                                                     |
| --------------- | ------------------------------------------------------------ |
| int waitStatus  | 表示节点的状态。其中包含的状态有：CANCELLED，值为1，由于在同步队列中等待的线程等待超时或者被中断，需要从队列中取消等待，节点进入该状态后将不会变化；SIGNAL，值为-1，表示当前节点的后继节点包含的线程处于等待状态，而如果当前线程如果释放了同步状态或者被取消，将**通知后继节点的线程得以运行**也就是unpark；CONDITION，值为-2，表示当前节点在**等待Condition**，当其他线程对Condition调用了Signal()方法后，该节点将会**从condition队列中转移到CLH队列**，加入到对同步状态的获取中；PROPAGATE，值为-3，表示当前场景下后续的acquireShared(共享同步状态获取)将会无条件地被传播下去；值为0，初始状态，表示当前节点在sync队列中，等待着获取锁。 |
| Node prev       | 前驱节点，比如当前节点被取消，那就需要前驱节点和后继节点来完成连接。 |
| Node next       | 后继节点。                                                   |
| Node nextWaiter | 存储condition队列(条件等待队列)中的后继节点。如果当前节点是共享的，那这个字段将是一个SHARED常量，即节点类型(独占和共享)和条件等待队列中的后继节点共用同一个字段。 |
| Thread thread   | 入队时获取同步状态的线程                                     |



Node成为sync队列和condition队列构建的基础，在同步器中就包含了sync队列。同步器拥有首节点和尾节点，没成功获取的线程就会加入该队列的尾部。同步队列基本结构如下：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1624803093229-db75c97a-484d-477a-8ae1-b92c4d2e793d.png)



1.当一个线程成功获取了同步状态(锁)，其他线程将无法获取到同步状态，转而被构造成了一个Node并加入到同步队列中，而这个过程必须保证线程安全性，为此，同步器提供了一个基于CAS的设置尾节点的方法：compareAndSetTail(Node expect, Node update)

2.当首节点的线程在释放同步状态时，将会唤醒后继节点，而后继节点将会在获取同步状态成功时将自己设置为首节点，由于只有一个线程能成功获取到同步状态，因此不需要CAS操作，只需将head的next节点去掉原来的首节点并设置原首节点的后继节点为新的首节点即可。



### 独占式锁的获取

API说明：

| 方法名称                              | 描述                                                         |
| ------------------------------------- | ------------------------------------------------------------ |
| protected boolean tryAcquire(int arg) | 排它的获取这个状态。这个方法的实现需要查询当前状态是否允许获取，然后再进行获取（使用compareAndSetState来做）状态。 |
| protected boolean isHeldExclusively() | 在排它模式下，状态是否被占用。                               |

#### acquire()

此方法是**独占模式**下线程获取共享资源的顶层入口。

如果获取到资源，线程直接返回，否则进入同步等待队列，直到获取到资源为止，且整个过程忽略中断的影响。这也正是lock()的语义，当然不仅仅只限于lock()。获取到资源后，线程就可以去执行其临界区代码了。下面是acquire()的源码：

```
public final void acquire(int arg) {
        if (!tryAcquire(arg) &&
            acquireQueued(addWaiter(Node.EXCLUSIVE), arg))
            selfInterrupt();
}
```

函数流程如下：

1. tryAcquire()尝试直接去获取资源，如果成功则直接返回true；
2. 如果获取失败，addWaiter()将该线程加入同步等待队列的尾部，并标记为独占模式；

1. acquireQueued()使线程在同步等待队列中获取资源：

1. 获取成功，节点出队，并且head去掉原首节点，且设置next为原首节点的后继节点，即当前节点
2. 获取失败，阻塞等待被唤醒。（**相关signal状态值会更改**）

1. 如果线程在等待过程中被中断过，它是不响应的。只是获取资源后才再进行自我中断selfInterrupt()，将中断补上。

####  tryAcquire(arg)

尝试直接去获取资源，如果成功则直接返回true，否则返回false。就像Lock接口提供的方法tryLock()方法的语义一样。

```
protected boolean tryAcquire(int arg) {
    throw new UnsupportedOperationException();
}
```

AQS这里只定义了一个接口，具体资源的获取交由自定义同步器去实现，具体的堵塞、重入等特性需要自行定义。

非abstract，可自行**选择模式(独占或共享)**开发接口。



#### addWaiter

获取失败，addWaiter()将该线程加入同步等待队列的尾部，并标记为独占模式。

```java
private Node enq(final Node node) {
    //自旋，直到成功加入队尾
    for (;;) {
        Node t = tail;
        if (t == null) { // Must initialize
            //原子操作初始化头节点
            if (compareAndSetHead(new Node()))
                tail = head;
        } else {
            node.prev = t;
            //原子操作进入队尾（替换尾节点）
            if (compareAndSetTail(t, node)) {
                t.next = node;
                return t;
            }
        }
    }
}
private Node addWaiter(Node mode) {
    //指定模式构造节点 独占或共享式
    Node node = new Node(Thread.currentThread(), mode);
    // Try the fast path of enq; backup to full enq on failure
    //快速入队
    Node pred = tail;
    if (pred != null) {
        node.prev = pred;
        if (compareAndSetTail(pred, node)) {
            pred.next = node;
            return node;
        }
    }
    enq(node);
    return node;
}
```



#### acquireQueued

前几步已经表明当前线程获取锁失败，且加入到队列尾部了。那么接下来就需要当前线程进入休眠等待期，等待被唤醒，才能正常的获取资源。

```java
final boolean acquireQueued(final Node node, int arg) {
    //线程是否获取到锁
    boolean failed = true;
    try {
        //是否被中断
        boolean interrupted = false;
        //自旋
        for (;;) {
            //获取当前节点的 前驱节点
            final Node p = node.predecessor();
            //如果前驱节点是头节点，代表当前线程所在的node在队列第一个，有资格尝试获取锁
            if (p == head && tryAcquire(arg)) {
                //获取成功，则需要将当前线程的node设置为head节点
                setHead(node);
                //将原head节点的next置null，代表原来的头节点脱离队列
                p.next = null; // help GC
                failed = false;
                return interrupted;
            }
            //获取失败，阻塞等待，直到被唤醒unpark
            //1进入等待
            //2判断是否被中断唤醒，且标记被中断过
            if (shouldParkAfterFailedAcquire(p, node) &&
                parkAndCheckInterrupt())
                interrupted = true;
        }
    } finally {
        //获取失败，取消尝试获取
        if (failed)
            cancelAcquire(node);
    }
}
```



```java
private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) {
    int ws = pred.waitStatus;
    if (ws == Node.SIGNAL)
      //前驱节点状态是signal（处理完后会通知自己），可以安全的继续等待
        return true;
    if (ws > 0) {
        do {
            //去掉取消了的节点，往后找最近一个状态正常的节点
            node.prev = pred = pred.prev;
        } while (pred.waitStatus > 0);
        pred.next = node;
    } else {
       //当前驱节点waitStatus是0或者propagate时，将它设置为signal，然后才能安全地等待。
        compareAndSetWaitStatus(pred, ws, Node.SIGNAL);
    }
    return false;
}

private final boolean parkAndCheckInterrupt() {
    //阻塞，底层调用系统内核功能
    LockSupport.park(this);
    //返回当前线程的中断状态
    return Thread.interrupted();
}
```

park()会让当前线程进入waiting状态。在此状态下，有两种途径可以唤醒该线程：1）被unpark()；2）被interrupt()。



这时引入一个释放的问题，也就是说**使睡眠中的Node或者说线程获得通知**的关键，就是**前驱节点的通知**，而这一个过程就是释放，释放会通知它的后继节点从睡眠中返回准备运行。

### 独占式锁的释放

| 方法名称                              | 描述       |
| ------------------------------------- | ---------- |
| protected boolean tryRelease(int arg) | 释放状态。 |

#### release(int arg)

acquire方法是保证能够获取到锁，即修改锁的状态。相反，release就是将锁状态设置回去，即释放锁，释放资源。而且此方法是**独占模式下**线程释放共享资源的顶层入口（各自行实现同步器下）

```java
public final boolean release(int arg) {
    //释放一次锁成功？成功->原子化的将状态设置回去
    if (tryRelease(arg)) {
        Node h = head;
        if (h != null && h.waitStatus != 0)
            //唤醒后继节点
            unparkSuccessor(h);
        return true;
    }
    return false;
}
```

#### tryRelease(int arg)

```
//释放锁资源，具体在子类中实现
protected boolean tryRelease(int arg) {
    throw new UnsupportedOperationException();
}
```



#### unparkSuccessor(Node node)

```java
private void unparkSuccessor(Node node) {
    //当前线程对应node在队列中的状态
    int ws = node.waitStatus;
    if (ws < 0)
        //释放锁，设置状态为0，保证以后还能尝试继续访问一次
        compareAndSetWaitStatus(node, ws, 0);

    //若持有锁当前线程的后继节点为空，或者状态为取消时，需要从队列尾部开始往前遍历寻找状态
    //正常的节点线程
    Node s = node.next;
    if (s == null || s.waitStatus > 0) {
        s = null;
        //为什么从尾部开始遍历？
        //场景：当方法执行到此处时，可能确实没有后继节点，但也就是在此时，别的线程addWaiter进来
        //了队列尾部，那tail!=null,防止以为没有后继节点，导致唤醒失败
        for (Node t = tail; t != null && t != node; t = t.prev)
            if (t.waitStatus <= 0)
                s = t;
    }
    //唤醒s线程，即状态ok的线程
    if (s != null)
        LockSupport.unpark(s.thread);
}
```



### 共享式锁的获取

API说明：

| 方法名称                                | 描述                   |
| --------------------------------------- | ---------------------- |
| protected int tryAcquireShared(int arg) | 共享的模式下获取状态。 |

#### acquireShared(int arg)

此方法是**共享模式**下线程获取共享资源的顶层入口。

共享模式和独占模式是有所区别的：如文件的读写操作，当某个文件，被线程1读取的时候，其它线程也是可以读取的，但是当某个线程在写文件操作的时候，在这一时刻别的线程的读写操作都会被堵塞，直到写操作完成。

```java
public final void acquireShared(int arg) {
    if (tryAcquireShared(arg) < 0)
        doAcquireShared(arg);
}
```

大概逻辑：

1.调用tryAcquireShared方法尝试获取共享锁，如果成功则直接返回，处理自己的逻辑。

2.调用tryAcquireShared方法失败会小于0，则调用doAcquireShared以共享模式加入同步队列中，并等待，拿到资源才返回。



#### tryAcquireShared(int arg)

与tryAcquire方法类似，都需要子类自己实现

```
//1.当返回值大于0时，表示获取同步状态成功，同时还有剩余同步状态可供其他线程获取； 
//2.当返回值等于0时，表示获取同步状态成功，但没有可用同步状态了； 
//3.当返回值小于0时，表示获取同步状态失败。 
protected int tryAcquireShared(int arg) {
    throw new UnsupportedOperationException();
}
```

#### doAcquireShared(int arg)

```java
private void doAcquireShared(int arg) {
    //以共享模式加入同步队列中
    final Node node = addWaiter(Node.SHARED);
    //标记获取锁成功与否的状态
    boolean failed = true;
    try {
        //中断状态
        boolean interrupted = false;
        //自旋
        for (;;) {
            //获取前驱节点
            final Node p = node.predecessor();
            //前驱为头节点时，
            if (p == head) {
                //则可以再一次尝试获取共享锁
                int r = tryAcquireShared(arg);
                //大于0说明还有同步状态可以选择
                if (r >= 0) {
                    //设置node为头节点，如果有下一节并没到安全点时，需设置为传播模式
                    setHeadAndPropagate(node, r);
                    p.next = null; // help GC
                    if (interrupted)
                        selfInterrupt();
                    failed = false;
                    return;
                }
            }
            
            //达到安全点等待，见上面分析
            if (shouldParkAfterFailedAcquire(p, node) &&
                parkAndCheckInterrupt())
                interrupted = true;
        }
    } finally {
        if (failed)
            cancelAcquire(node);
    }
}
```

#### setHeadAndPropagate

```java
private void setHeadAndPropagate(Node node, int propagate) {
    //存放旧的头节点
    Node h = head; 
    //设置node为新的头节点
    setHead(node);
    
    //1。propagate > 0表示调用的线程指明了后继节点需要被唤醒才行
    //2。头节点的后继节点需要被唤醒，不论是旧的还是新的头节点。
    if (propagate > 0 || h == null || h.waitStatus < 0 ||
        (h = head) == null || h.waitStatus < 0) {
        Node s = node.next;
        //node是尾节点，或者 node的后继节点是共享模式的节点
        if (s == null || s.isShared())
            doReleaseShared();
    }
}

final boolean isShared() {
    return nextWaiter == SHARED;
}
```



#### doReleaseShared()

似懂非懂！！！

```java
private void doReleaseShared() {
    //自旋
    for (;;) {
        Node h = head;
        //一定有后继节点
        if (h != null && h != tail) {
            int ws = h.waitStatus;
            //head状态为signal时，会重置为0，不会直接设置PROPAGATE，因为有两个地方可以
            //进行unpark，当前方法setHeadAndPropagate和release方法，避免重复唤醒
            //
            if (ws == Node.SIGNAL) {
                if (!compareAndSetWaitStatus(h, Node.SIGNAL, 0))
                    continue;//设置失败，重新走循环    
                //设置成功，唤醒head.next节点的线程，此时锁如果被head.next获取，则head会指向
                //当前获取锁的node，即head变了。
                unparkSuccessor(h);
            }
            // 如果head节点的状态为0，需要设置为PROPAGATE，表示将状态向后继节点传播。
            else if (ws == 0 &&
                     !compareAndSetWaitStatus(h, 0, Node.PROPAGATE))
                continue;                // loop on failed CAS
        }
        if (h == head)                   // head变了 重新循环
            break;
    }
}
```



### 共享式锁的释放

| 方法名称                                    | 描述                   |
| ------------------------------------------- | ---------------------- |
| protected boolean tryReleaseShared(int arg) | 共享的模式下释放状态。 |

#### releaseShared(int arg)

释放指定量的资源，如果成功释放且允许唤醒等待线程，它会唤醒等待队列里的其他线程来获取资源。

```java
public final boolean releaseShared(int arg) {
    if (tryReleaseShared(arg)) {
        doReleaseShared();
        return true;
    }
    return false;
}
```

#### tryReleaseShared(int arg)

尝试去释放指定量的共享锁，也是子类自行实现逻辑。

```java
protected boolean tryReleaseShared(int arg) {
    throw new UnsupportedOperationException();
}
```

doReleaseShared()在共享锁的获取时讲过，可以看出，共享锁的获取和释放都会涉及到doReleaseShared,也就是后继线程的唤醒。

## 框架实现

列举几个常见的AQS实现框架

### Semaphore

#### 简介

semaphore也是由Doug Lea所编写的，其字面翻译是信号的意思。正如源码所备注的意思，它是通过一个内部类实现AQS，用AQS的state变量(**volatile int state,同步状态量**)指定访问线程数来控制并发访问的。**本质上是用的共享锁模式**。



#### 结构API说明

1.通过内部类Sync继承AQS，覆写相关方法

2.一样有公平锁和非公平锁的实现



**构造方法**

| 方法名称                                    | 描述                                                   |
| ------------------------------------------- | ------------------------------------------------------ |
| public Semaphore(int permits)               | 创建一个指定的可并发访问数量的信号量，默认非公平实现。 |
| public Semaphore(int permits, boolean fair) | 创建一个指定的可并发访问数量的信号量，并指定公平性。   |



**重要方法**

| 方法名称                                                     | 描述                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| public void acquireUninterruptibly(int permits) {     if (permits < 0) throw new IllegalArgumentException();     sync.acquireShared(permits); } | 获取指定的访允许量，当信号量还有指定的允许数目时则拿到，否则就一直堵塞。**如果中断了不会报错，自旋到中断node会进行自我中断** |
| public void acquire() throws InterruptedException {     sync.acquireSharedInterruptibly(1); } | 获取访问资格，当信号量还有允许时则拿到，否则就堵塞，直到有一个允许时，或者被中断才停止。(**中断直接异常**)acquireSharedInterruptibly->在共享模式下获取，如果中断则中止。具体看AQS代码。 |
| public void acquire(int permits) throws InterruptedException {     if (permits < 0) throw new IllegalArgumentException();     sync.acquireSharedInterruptibly(permits); } | 获取指定的访允许量，当信号量还有指定的允许数目时则拿到，否则就一直堵塞。**如果中断了直接异常** |
| public void release(int permits) {     if (permits < 0) throw new IllegalArgumentException();     sync.releaseShared(permits); } | 释放指定的允许数量                                           |



#### 使用

**场景**

**通常用于那些资源有明确访问数量限制的场景，常用于限流** 。

比如：数据库连接池，同时进行连接的线程有数量限制，连接不能超过一定的数量，当连接达到了限制数量后，后面的线程只能排队等前面的线程释放了数据库连接才能获得数据库连接。

比如：停车场场景，车位数量有限，同时只能容纳多少台车，车位满了之后只有等里面的车离开停车场外面的车才可以进入。



**简单案例**

```
package com.gx.demo.concurrent;

import java.util.concurrent.Semaphore;

public class SemaphoreTest {

    public static void main(String[] args) {
        Semaphore semaphore = new Semaphore(3);
        for (int i = 0; i < 10; i++) {
            new Thread(new MyTask(semaphore)).start();
        }
    }

    static class MyTask extends Thread {
        private Semaphore semaphore;

        public MyTask(Semaphore semaphore){
            this.semaphore = semaphore;
        }

        @Override
        public void run() {
            try {
                semaphore.acquire();
                System.out.println(Thread.currentThread().getName() + ":the time: " + System.currentTimeMillis());
                Thread.sleep(3000);
                semaphore.release();
                System.out.println(Thread.currentThread().getName() + ":the time: " + System.currentTimeMillis());
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }
    }

}
```

**结果**

**一次只能有3个线程能够执行逻辑，其它的必须等线程释放了之后才能尝试获取执行。**

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625752717939-04cb1d48-502f-4f0e-a561-2494be9a7651.png)

### CountDownLatch

#### 简介

CountDownLatch也是由Doug Lea所编写的，其字面翻译是**倒计时锁存器**的意思。

正如源码所备注的 "*A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes*" 。指的是：一种同步辅助，允许一个或多个线程等待，直到在其他线程中执行的一组操作完成。



#### 结构API说明

1.通过内部类Sync继承AQS，覆写相关方法。主要通过一个计数器state来实现的，计数器的初始值即为线程可获取的数量。每当一个线程完成了自己的任务，state就会减1。直到state为0时，表示所有的线程都完成了任务，才会执行await等待后面的逻辑。

2.共享模式实现

3.无公平、非公平特性





**构造方法**

| 方法名称                         | 描述                |
| -------------------------------- | ------------------- |
| public CountDownLatch(int count) | 初始化指定state变量 |



**重要方法**

| 方法名称                                                     | 描述                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| public void await() throws InterruptedException {     sync.acquireSharedInterruptibly(1); } | 当前线程会一直等待，知道latch的state置0(每次countDown()都会减1)，除非被中断。**如果中断直接异常** |
| public boolean await(long timeout, TimeUnit unit)     throws InterruptedException {     return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout)); } | 当前线程会一直等待，知道latch的state置0(每次countDown()都会减1)，除非被中断或者线程等待时间超过了指定等待时间。**如果中断直接异常** |
| public void countDown() {     sync.releaseShared(1); }       | 共享模式下释放锁，这里指定每次释放一个。                     |



#### 使用

**场景**

如果需要执行一个任务，但是必须先等其他几个任务做到某个程度这个任务才能够启动，这个时候就比较适合用CountDownLatch。但是一般会使用超时等待的方式来处理，不然如果其中某个任务异常没有完成，或者超时了，那么任务会一直等待在那里。

比如：某个任务是：同时需要几个人去不同的商店排队买商品，都买完了才算通过这个关卡进行下一关。



**简单案例**

```
package com.gx.demo.concurrent;
import java.util.concurrent.CountDownLatch;

public class CountDownLatchTest {

    public static void main(String[] args) throws InterruptedException {
        CountDownLatch countDownLatch = new CountDownLatch(2);
        new Thread(() -> {
            System.out.println("a");
            System.out.println(countDownLatch.getCount());
            countDownLatch.countDown();
            System.out.println(countDownLatch.getCount());
            System.out.println("b");
            //countDownLatch.countDown();
            System.out.println(countDownLatch.getCount());
        }).start();

        countDownLatch.await();
        System.out.println("c");
    }
}
```

**结果1**

**state只消费了一次，且线程堵塞**

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625756708784-00c827f1-12b8-476b-92d4-c6e0d01f8b15.png)

**结果2**

**state只消费了两次，且线程不堵塞**

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625756741433-0db824c0-d109-489b-85ea-d625462c7abd.png)



### CyclicBarrie

#### 简介

CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。

它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。



#### 结构API说明

**构造方法**

| 方法名称                                                     | 描述                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| public CyclicBarrier(int parties)                            | 其参数表示屏障拦截的线程数量，每个线程调用await方法告诉CyclicBarrier我已经到达了屏障，然后当前线程被阻塞 |
| public CyclicBarrier(int parties, Runnable **barrierAction**) | 与上面类似，不过当线程达到屏障时会去优先执行barrierAction的逻辑 |



**重要方法**

| 方法名称                                                     | 描述                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| public int await() throws InterruptedException, BrokenBarrierException {     try {         return dowait(false, 0L);     } catch (TimeoutException toe) {         throw new Error(toe); // cannot happen     } } | 一直等到除非所有成员都调用了await方法到达了障碍上。如果当前线程不是最后一个到达，那么它是出于线程调度目的而禁用并处于休眠状态，直到发生以下情况之一：1最后一个线程到达；2其他一些线程interrupt当前线程；3其他一些线程interrupt其他等待线程之一；4其他一些线程在等待屏障时超时；5某些其他线程在此屏障上调用 reset。 |
| public int await(long timeout, TimeUnit unit)                | 与上类似，一直等到除非所有成员都调用了await方法到达了障碍上，或者指定等待时间超时了。 |
| public void reset() {     final ReentrantLock lock = this.lock;     lock.lock();     try {         breakBarrier();   // 将当前屏障生成设置为已破坏并唤醒所有人。(仅在持有锁时调用)         nextGeneration(); // start a new generation     } finally {         lock.unlock();     } } | 将障碍重置为其初始状态。如果目前在屏障处等待的任何成员，将返回BrokenBarrierException异常。在重置之后，由于其他原因发生的破损可能执行起来很复杂;线程需要以其他方式重新同步，并选择一个来执行重置。可能更可取的是为后续使用的创建一个新的屏障。 |
| /** The lock for guarding barrier entry */ private final ReentrantLock lock = new ReentrantLock();/** Condition to wait on until tripped */ private final Condition trip = lock.newCondition(); | 内置了可重入锁，以及一个条件condition用的是AQS的条件队列实现 |



#### 使用

**场景**

CyclicBarrier可以用于多线程计算数据，最后合并计算结果的应用场景。

比如：财务业务计算总利润，需要先算出总支出、总成本等等。



**简单案例**

```
static CyclicBarrier barrier = new CyclicBarrier(2);

public static void main(String[] args) {
    new Thread(() -> {
        try {
            barrier.await();
            System.out.println(String.format("调用第%s", "A"));
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (BrokenBarrierException e) {
            e.printStackTrace();
        }
    }).start();

    try {
        barrier.await();
        System.out.println(String.format("调用第%s", "B"));
    } catch (InterruptedException e) {
        e.printStackTrace();
    } catch (BrokenBarrierException e) {
        e.printStackTrace();
    }
}
```

**结果**两种输出：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625794861512-18ca6941-22f3-4a5c-bb1b-e78e10100ba3.png)![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625794941163-022f9f75-a156-4ef6-82bf-a6ec55861042.png)



有优先执行逻辑的时候：

```
static CyclicBarrier barrier = new CyclicBarrier(2, new MyTask());

public static void main(String[] args) {
    new Thread(() -> {
        try {
            barrier.await();
            System.out.println(String.format("调用第%s", "A"));
        } catch (InterruptedException e) {
            e.printStackTrace();
        } catch (BrokenBarrierException e) {
            e.printStackTrace();
        }
    }).start();

    try {
        barrier.await();
        barrier.reset();
        System.out.println(String.format("调用第%s", "B"));
    } catch (InterruptedException e) {
        e.printStackTrace();
    } catch (BrokenBarrierException e) {
        e.printStackTrace();
    }
}

static class MyTask implements Runnable {

    @Override
    public void run() {
        System.out.println(String.format("调用第%s", "C"));
    }
}
```

**结果**

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1625795978044-4de8e4db-e0a6-4040-89b8-bd6207d67edd.png)



## BlockingQueue 

### 简介 

BlockingQueue（堵塞队列），其位于java.util.concurrent包下，也是由Doug lea大佬所编写。

正如源码注释所解释的：

1.它实现旨在主要用于生产者-消费者队列，但也支持java.util.Collection接口。

2.它可以容量有界，也可以没有界限(Integer.MAX_VALUE)。

3.它是线程安全的，可以在任意时刻只有一个线程能进行take、put操作。

4.提供了超时return null的机制



### 常用API说明

#### 队列元素添加

| 方法名称                                                     | 描述                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| boolean add(E e);                                            | 插入指定元素，如果插入成功则返回true，否则IllegalStateException异常(队列当前没空余空间)。 |
| void put(E e) throws InterruptedException;                   | 插入指定元素，当队列满了时，会堵塞直到有空间可以插入队列(当等待时中断了会抛异常)。 |
| boolean offer(E e);                                          | 插入指定元素，如果成功则返回true，否则返回false。一般在有界队列中，更偏向用add方法。 |
| boolean offer(E e, long timeout, TimeUnit unit)     throws InterruptedException; | 尝试插入指定元素，当队列满了时，如果有必要，会堵塞住，并等待指定的等待时间来加入队列。 |

#### 队列元素获取

| 方法名称                                                     | 描述                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| E take() throws InterruptedException;                        | 返回并删除队列头部元素，如果队列为空，将堵塞直到有一个可用元素能够被获取。 |
| E poll(long timeout, TimeUnit unit)     throws InterruptedException; | 返回并删除队列头部元素，如果队列为空，将堵塞并在指定的等待时间内获取到有一个可用元素能够才返回，否则返回null。 |



### 队列类型

两种类型的BlockingQueue：

- unbounded queue – 可以无限增长(Integer.MAX_VALUE)
- bounded queue – 指定最大容量



### 常见实现框架(数据结构)

就像mysql的索引实现一样，队列的实现本质也是依靠某一种数据结构实现的。

1.一般是用数组或者链表实现，且队列其主要操作就是入队和出队。

2.队列可具有的一些特性：FIFO、优先、延迟、双端等。

列觉几种常见的堵塞队列：

- ArrayBlockingQueue：数组实现的有界队列
- DelayQueue：优先级堆实现的基于时间的调度队列

- LinkedBlockingQueue：链表实现的可选有界队列
- PriorityBlockingQueue：优先级堆实现的无界队列



#### ArrayBlockingQueue

**简介**：

ArrayBlockingQueue 是一个**线程安全的、基于数组、有界的、阻塞的、FIFO** 队列。试图向已满队列中放入元素会导致操作受阻塞；试图从空队列中提取元素将导致类似阻塞。

此类基于ReentrantLock 来实现线程安全，所以提供了 ReentrantLock 所能支持的**公平性选择**。



**重要属性**：

```
// 队列存放元素的容器，决定了数组大小(队列大小)不可变
final Object[] items;

//并没有使用volatile修饰，因为访问这些变量使用都是在锁块内，并不存在可见性问题
// 下一次读取或移除的位置(出队)
int takeIndex;

// 存放下一个放入元素的位置(入队)
int putIndex;

// 队列里有效元素的数量
int count;

// 所有访问的保护锁
final ReentrantLock lock;

//两个条件对象用于协调队列的出队和入队操作
// 等待获取的条件
private final Condition notEmpty;

// 等待放入的条件
private final Condition notFull;
```



**构造方法**：

```
//指定容量，默认非公平锁
public ArrayBlockingQueue(int capacity) {
    this(capacity, false);
}
//指定容量和锁类别
public ArrayBlockingQueue(int capacity, boolean fair) {
    if (capacity <= 0)
        throw new IllegalArgumentException();
    this.items = new Object[capacity];
    lock = new ReentrantLock(fair);
    notEmpty = lock.newCondition();
    notFull =  lock.newCondition();
}
```

**其它方法**：

```
//入队
private void enqueue(E x) {
    // assert lock.getHoldCount() == 1;
    // assert items[putIndex] == null;
    final Object[] items = this.items;
    items[putIndex] = x;//初始0
    //每次都+1给到下一个要入队的元素，满了就置0，可以看成是一个环形的存储方式
    if (++putIndex == items.length)
        putIndex = 0;
    count++;//队列容量+1
    notEmpty.signal();//不为空条件唤醒
}
//出队
private E dequeue() {
    // assert lock.getHoldCount() == 1;
    // assert items[takeIndex] != null;
    final Object[] items = this.items;
    @SuppressWarnings("unchecked")
    E x = (E) items[takeIndex];
    items[takeIndex] = null;
    //每次都+1给到下一个要出队的元素，满了也置0，可以看成是一个环形的存储方式
    if (++takeIndex == items.length)
        takeIndex = 0;
    count--;
    if (itrs != null)//链表形式管理的一个或多个迭代器
        itrs.elementDequeued();
    notFull.signal();//队列没满条件唤醒
    return x;
}
```

可参考：[http://ifeve.com/%e5%b9%b6%e5%8f%91%e9%98%9f%e5%88%97-%e6%9c%89%e7%95%8c%e9%98%bb%e5%a1%9e%e9%98%9f%e5%88%97arrayblockingqueue%e5%8e%9f%e7%90%86%e6%8e%a2%e7%a9%b6/](http://ifeve.com/并发队列-有界阻塞队列arrayblockingqueue原理探究/)



#### LinkedBlockingQueue

**简介**：

LinkedBlockingQueue是**基于链表**实现的、**线程安全的、可选有界的**(可以指定容量)**、阻塞的、FIFO** 队列，也是依托独占锁实现的阻塞队列。



**重要属性**：

```
//容量大小，最大Integer.MAX_VALUE
private final int capacity;

//当前队列元素总数
private final AtomicInteger count = new AtomicInteger();

//链表(队列)头部元素 head.item == null
transient Node<E> head;

//链表(队列)尾部元素 last.next == null
private transient Node<E> last;

//出队锁 take, poll
private final ReentrantLock takeLock = new ReentrantLock();

//等待获取元素条件
private final Condition notEmpty = takeLock.newCondition();

//入队锁 put, offer
private final ReentrantLock putLock = new ReentrantLock();

//等待添加元素条件
private final Condition notFull = putLock.newCondition();
```

**构造方法**：

```
public LinkedBlockingQueue() {
    this(Integer.MAX_VALUE);//默认大小
}
//指定容量
public LinkedBlockingQueue(int capacity) {
    if (capacity <= 0) throw new IllegalArgumentException();
    this.capacity = capacity;
    //链表头部尾部节点初始为null
    last = head = new Node<E>(null);
}
```

**其它方法**：

```
private void enqueue(Node<E> node) {
	//直接放到最后，但容量count没在此处增加，在具体的入队方法中进行了 原子性增加/减少
    last = last.next = node;
}

private E dequeue() {
    Node<E> h = head;
    Node<E> first = h.next;
    h.next = h; // help GC
    head = first;
    E x = first.item;
    first.item = null;
    return x;
}
```

#### PriorityBlockingQueue

**简介**：

PriorityBlockingQueue是一个依托数组、可重入锁、优先级堆、**可选有界的**(可以指定容量)队列。



**重要属性**：

```
private static final int DEFAULT_INITIAL_CAPACITY = 11;//默认容量
//数组最大大小，可能和grow有关
private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;
private transient Object[] queue;//存放数据队列
private transient Comparator<? super E> comparator;//比较器
private final ReentrantLock lock;//锁控制每次只有一个线程操作
private final Condition notEmpty;//实现take获取元素堵塞，无notFull因为是无界的
//构建堆时使用，用于分配的自旋锁，通过 CAS 获取。
private transient volatile int allocationSpinLock;
private PriorityQueue<E> q;//序列化时使用
```

**构造方法**：

```
public PriorityBlockingQueue() {
    this(DEFAULT_INITIAL_CAPACITY, null);//默认11
}
public PriorityBlockingQueue(int initialCapacity) {
    this(initialCapacity, null);//指定容量大小
}
//指定容量大小 以及 自定义比较器定义优先级的规则
public PriorityBlockingQueue(int initialCapacity,
                             Comparator<? super E> comparator) {
    if (initialCapacity < 1)
        throw new IllegalArgumentException();
    this.lock = new ReentrantLock();
    this.notEmpty = lock.newCondition();
    this.comparator = comparator;
    this.queue = new Object[initialCapacity];
}
```

**其它方法**：

```
//入队
//public boolean add(E e) 和 put 都用的offer方法
public boolean offer(E e) {
    if (e == null)//空元素报错
        throw new NullPointerException();
    final ReentrantLock lock = this.lock;
    lock.lock();
    int n, cap;//后面接收用
    Object[] array;
    //如果当前队列大小>=队列长度 则尝试扩容
    while ((n = size) >= (cap = (array = queue).length))
        tryGrow(array, cap);
    try {
        Comparator<? super E> cmp = comparator;
        //如果没自定义比较器就走默认的规则，否则用自己的规则
        if (cmp == null)
            //n队列元素数量，e入队元素，array队列
            siftUpComparable(n, e, array);
        else
            siftUpUsingComparator(n, e, array, cmp);
        size = n + 1;
        notEmpty.signal();
    } finally {
        lock.unlock();
    }
    return true;
}

private void tryGrow(Object[] array, int oldCap) {
    //先把锁释放了，可能扩容时间久，会影响性能？
    lock.unlock(); // must release and then re-acquire main lock
    Object[] newArray = null;
    //当扩容的时候，把allocationSpinLockCAS为1，防止别的线程同时修改
    if (allocationSpinLock == 0 &&
        UNSAFE.compareAndSwapInt(this, allocationSpinLockOffset,
                                 0, 1)) {
        try {
            //1.如果当前队列长度<64,则扩容为 old+(old+2)
            //2.如果当前队列长度>=64,则扩容为 old+(old/2)50%
            int newCap = oldCap + ((oldCap < 64) ?
                                   (oldCap + 2) : // grow faster if small
                                   (oldCap >> 1));
            if (newCap - MAX_ARRAY_SIZE > 0) {    // possible overflow
                int minCap = oldCap + 1;
                if (minCap < 0 || minCap > MAX_ARRAY_SIZE)
                    throw new OutOfMemoryError();
                newCap = MAX_ARRAY_SIZE;
            }
            if (newCap > oldCap && queue == array)
                newArray = new Object[newCap];
        } finally {
            allocationSpinLock = 0;//扩容完，重新置0
        }
    }
    //没拿到扩容资格的线程，让出cpu使用权
    if (newArray == null) // back off if another thread is allocating
        Thread.yield();
    //从新加锁，给到之前拿到扩容权限的线程
    lock.lock();
    //把扩容后的数组(队列) 更新到全局队列，因为重新加了锁，队列不会再变化了。
    if (newArray != null && queue == array) {
        queue = newArray;
        System.arraycopy(array, 0, newArray, 0, oldCap);
    }
}

private static <T> void siftUpComparable(int k, T x, Object[] array) {
    //队列元素总数
    Comparable<? super T> key = (Comparable<? super T>) x;
    //大于0则搭建堆
    while (k > 0) {
        int parent = (k - 1) >>> 1;//
        Object e = array[parent];
        //判断队列总数是否大于插入的元素(key<64,每次2key+2)
        if (key.compareTo((T) e) >= 0)
            break;
        array[k] = e;
        k = parent;
    }
    array[k] = key;
}
private E dequeue() {
    int n = size - 1;
    //队列为空，则返回null
    if (n < 0)
        return null;
    else {
        Object[] array = queue;
        //获取队头元素
        E result = (E) array[0];
        //获取对尾元素，并设置值null
        E x = (E) array[n];
        array[n] = null;
        Comparator<? super E> cmp = comparator;
        if (cmp == null)
            siftDownComparable(0, x, array, n);
        else
            siftDownUsingComparator(0, x, array, n, cmp);
        size = n;
        return result;
    }
}
```



#### DelayQueue

**简介**：

由优先级堆支持的、基于时间的调度队列，内部基于无界队列PriorityQueue实现，而无界队列基于数组的扩容实现。 

 	 

**重要属性**：

```
//可重入锁控制
private final transient ReentrantLock lock = new ReentrantLock();
//优先级队列
private final PriorityQueue<E> q = new PriorityQueue<E>();
//队列领导线程，指定等待队列头部元素的线程
private Thread leader = null;
//当在队列的头部的新元素可用时发出条件信号或一个新线程可能需要成为领导者。
private final Condition available = lock.newCondition();
```

**构造方法**：

```
//添加一个集合的元素
public DelayQueue(Collection<? extends E> c) {
    this.addAll(c);
}
```

**其它方法**：

```
//add、put都是基于offer方法，且没有超时的设定
public boolean offer(E e) {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        q.offer(e);
        //入队判断下是否自己是对头
        if (q.peek() == e) {
            leader = null;
            available.signal();
        }
        return true;
    } finally {
        lock.unlock();
    }
}
//offer是调用的 PriorityQueue的offer方法
//与PriorityBlockingQueue类似，采用优先堆处理
private void siftUpComparable(int k, E x) {
    Comparable<? super E> key = (Comparable<? super E>) x;
    //有元素则比较
    while (k > 0) {
        int parent = (k - 1) >>> 1;
        Object e = queue[parent];
        if (key.compareTo((E) e) >= 0)
            break;
        queue[k] = e;
        k = parent;
    }
    queue[k] = key;
}

public E take() throws InterruptedException {
    final ReentrantLock lock = this.lock;
    lock.lockInterruptibly();
    try {
        for (;;) {
           	//获取对头元素
            E first = q.peek();
            //第一次take为空，说明队列没元素，则自己进行堵塞；
            //当执行offer的时候，添加的元素就是对头时，就会唤醒最先等待的线程
            if (first == null)
                available.await();
            else {
                //看下对头元素还有多久到期
                long delay = first.getDelay(NANOSECONDS);
                //到期直接剔除，出队
                if (delay <= 0)
                    return q.poll();
                first = null; // don't retain ref while waiting
                //或者leader不为空，说明别的线程在操作，需堵塞住
                if (leader != null)
                    available.await();
                else {
                    //否则自己拿到资格，并设置leader为当前线程，
                    Thread thisThread = Thread.currentThread();
                    leader = thisThread;
                    try {
                        //等待设定的过期时间
                        available.awaitNanos(delay);
                    } finally {
                        //等待过期到了，则要释放leader
                        if (leader == thisThread)
                            leader = null;
                    }
                }
            }
        }
    } finally {
        if (leader == null && q.peek() != null)
            available.signal();
        lock.unlock();
    }
}

public E poll() {
    final ReentrantLock lock = this.lock;
    lock.lock();
    try {
        //拿对头元素
        E first = q.peek();
        //如果是空或者还没获取 直接返回null
        if (first == null || first.getDelay(NANOSECONDS) > 0)
            return null;
        else
            //出队
            return q.poll();
    } finally {
        lock.unlock();
    }
}
//PriorityQueue的poll方法
public E poll() {
    if (size == 0)
        return null;
    int s = --size;
    modCount++;
    E result = (E) queue[0];
    E x = (E) queue[s];
    queue[s] = null;
    if (s != 0)
        //堆的操作
        siftDown(0, x);
    return result;
}
```

**注**：加入DelayQueue的需要实现Delay接口

```
public static void main(String[] args) throws InterruptedException {
    DelayQueue<MyDelayed> delayQueue = new DelayQueue<MyDelayed>();

    MyDelayed element1 = new MyDelayed(900000000,"zz");
    MyDelayed element2 = new MyDelayed(1000,"dd");

    delayQueue.offer(element1);
    delayQueue.offer(element2);

    element1 =  delayQueue.take();
    System.out.println(element1);
}

static class MyDelayed implements Delayed {

    private final long delayTime; //延迟时间
    private final long expire;  //到期时间
    private String data;   //数据

    public MyDelayed(long delay, String data) {
        delayTime = delay;
        this.data = data;
        expire = System.currentTimeMillis() + delay;
    }

    @Override
    public long getDelay(TimeUnit unit) {
        return unit.convert(this.expire - System.currentTimeMillis() , TimeUnit.MILLISECONDS);
    }

    @Override
    public int compareTo(Delayed o) {
        return (int) (this.getDelay(TimeUnit.MILLISECONDS) -o.getDelay(TimeUnit.MILLISECONDS));
    }

}
```

# Atomic&Unsafe

### 说明



### 常用原子操作类



### unsafe魔法类



# 线程池

众所周知，一个进程下可以创建多个线程，而系统的资源也是有优先级、有限的，如果线程被无限制的创建，不仅会消耗系统的资源，还会降低系统的稳定性。因此在Java里，提出了“线程池”的概念，用其来对线程实现统一的分配、监控和调优等。



## 场景引入

在开发中，服务端需要接收来自外部的http请求(如服务器是tomcat)，而一个请求本质上会被分配一个线程来处理逻辑。而如果每次有新的请求就新创建一个线程给予分配的话，看起来问题不大还很方便，甚至不会存在竞争。

但是如果请求数量非常多，每个被分配的线程其实也没处理什么逻辑时间花销很小，还一直这么频繁的创建和销毁线程，势必会造成系统效率的降低，以及系统在处理创建、销毁新线程上花费大量系统资源，无法给线程达到好的复用等。



即：如何保证线程能够更好的被复用起来，不至于让系统花费大量资源去创建、销毁新线程？

显然这即是线程池存在的目的了。利用线程池，去更好的管理线程的生命周期。通过实现线程的复用，减少线程创建、销毁的开销。



什么时候使用线程池? 

- 单个任务处理时间比较短      
- 需要处理的任务数量很大 



线程池优势
重用存在的线程，减少线程创建，消亡的开销，提高性能，提高响应速度。

当任务到达时，任务可以不需要等到新线程创建就能立即执行。  

提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。 

 				

## 线程的打开方式(创建)

### new Thread

```
Thread thread = new Thread();
```

### 实现Runnable接口

```
class RunnableTest implements Runnable
```

### 实现Runnable接口

```
class CallTest implements Callable
```



## Executor&类结构

类结构图

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1627371280196-578d9800-405e-4c19-86a1-f24429ad9df7.png)

Executor为各定义线程池实现的顶级接口，其内部只提供了一个用于执行Runnable的方法。

```java
void execute(Runnable command);
```

### ExecutorService接口

一个Executor，提供管理终止和终止的方法，可以生成Future以跟踪一个或多个异步任务。而ExecutorService的作用就是给线程池定义具体的行为。

```java
//Executor定义的，执行Runnable
void execute(Runnable command);
//启动有序关闭，其中会执行以前提交的任务，但不再接受新任务。
void shutdown();
//停止所有正在执行的任务，并不再接受新任务。
List<Runnable> shutdownNow();
//判断executor是否关闭
boolean isShutdown();
//判断是否所有任务都被执行完了
boolean isTerminated();
//提交Runnable或Callable任务，并返回代表该任务的Future对象(异步执行的结果)
Future<?> submit(Runnable task/Callable<T> task);
```

## ThreadPoolExecutor

### 重要成员属性

```java
//它包含两 部分的信息: 线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount)，
//使用了Integer类型来保存，高3位保存runState，低29位保存workerCount(源码注释有说明)
private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));
//29
private static final int COUNT_BITS = Integer.SIZE - 3;
//1左移29位，即29个0-1，即29个1(about 500 million)
private static final int CAPACITY   = (1 << COUNT_BITS) - 1;

//获取运行状态
private static int runStateOf(int c)     { return c & ~CAPACITY; }
//获取有效活动的线程数
private static int workerCountOf(int c)  { return c & CAPACITY; }
//获取运行状态和线程池内有效线程的数量
private static int ctlOf(int rs, int wc) { return rs | wc; }
```

### 线程池状态

```java
//用高三位表示的状态，因为COUNT_BITS 29位，int32位
//-1即 111
private static final int RUNNING    = -1 << COUNT_BITS;
//0即 000
private static final int SHUTDOWN   =  0 << COUNT_BITS;
//1即 001
private static final int STOP       =  1 << COUNT_BITS;
//2即 010
private static final int TIDYING    =  2 << COUNT_BITS;
//3即 011
private static final int TERMINATED =  3 << COUNT_BITS;
```
