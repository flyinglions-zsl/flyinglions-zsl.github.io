---
title: 并发学习
date: 2021-06-09 22:29:23
tags: 
	- 并发
categories:
	- 并发
---

# CPU线程调度

## 算法概念

假设计算机只有一个 CPU ，则在任意时刻只能执行一条机器指令，每个线程只有获得 CPU 的使用权才能执行指令。 

- 所谓多线程的并发运行，其实是指从宏观上看，各个线程轮流获得 CPU 的使用权，分别执行各自的任务。 
- 在运行池中，会有多个处于就绪状态的线程在等待 CPU ，Java 虚拟机的一项任务就是负责线程的调度，线程调度是指按照特定机制为多个线程分配 CPU 的使用权。 



有两种调度模型：**分时调度模型**和**抢占式调度模型**。 

- 分时调度模型是指让所有的线程轮流获得 CPU 的使用权,并且平均分配每个线程占用的 CPU 的时间片这个也比 

较好理解。 

- Java 虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用 CPU ，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用 CPU 。处于运行状态的线程会一直运行，直至它不得不放弃 

CPU 。



## 线程饥饿概念

饥饿，一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。 

Java 中导致饥饿的原因： 

- 高优先级线程吞噬所有的低优先级线程的 CPU 时间。 
- 线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访 

问。 

- 线程在等待一个本身也处于永久等待完成的对象(比如调用这个对象的 wait 方法)，因为其他线程总是被持 

续地获得唤醒。



## 线程优化级概念

每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实 

现，这个实现是和操作系统相关的(OS dependent)。 

- 我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级 

是一个 int 变量(从1-10)，1 代表最低优先级，10 代表最高优先级。 

- Java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一 

般无需设置线程优先级。



## 进程

### 概念

计算机的核心是CPU，它承担了所有的计算任务，而操作系统是计算机的管理者，它负责任务的调度，资源的分配和管理，统领整个计算机硬件；应用程序是具有某种功能的程序，程序是运行于操作系统之上的。



**进程**是一个具有一定独立功能的程序在一个数据集上的一次动态执行的过程，是操作系统进行资源分配和调度的一个独立单位，是应用程序运行的载体。进程是一种抽象的概念，从来没有统一的标准定义。



进程一般由**程序，数据集合和进程控制块三部分**组成。

- 程序用于**描述进程要完成**的功能，是控制进程执行的**指令集**；
- 数据集合是程序在执行时**所需要的数据和工作区**；

- 程序控制块包含**进程的描述信息**和是进程存在的唯一标志。

进程具有的特征：

**动态性**：进程是程序的一次执行过程，是临时的，有生命期的，是动态产生，动态消亡的；

**并发性**：任何进程都可以同其他进程一起并发执行；

**独立性**：进程是系统进行资源分配和调度的一个独立单位；

**结构性**：进程由程序，数据和进程控制块三部分组成。



## 线程

### 概念

在早期的操作系统中并没有线程的概念，**进程是拥有资源和独立运行的最小单位**，也是程序执行的最小单位。任务调度采用的是时间片轮转的抢占式调度方式，而进程是任务调度的最小单位，每个进程有各自独立的一块内存，使得各个进程之间内存地址相互隔离。

后来随着计算机的发展，对CPU的要求越来越高，进程之间的切换开销较大，已经无法满足越来越复杂的程序的要求了。于是就发明了线程，线程是程序执行中一个单一的顺序控制流程，**是程序执行流的最小单元**，是处理器调度和分派的基本单位。**一个进程可以有一个或多个线程**，**各个线程之间共享程序的内存空间**(也就是所在进程的内存空间)。一个标准的线程由线程ID，当前指令指针PC，寄存器和堆栈组成。

而进程由内存空间(代码，数据，进程空间，打开的文件)和一个或多个线程组成。



### 生命周期

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1623554892415-b0bd3eab-2610-42be-b11d-8599e70d0d09.png)

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1623555150026-859acd76-66cc-4ee7-be64-eb5a767a1296.png)



# JMM模型

## 概念

**Java内存模型**(Java Memory Model简称JMM)是一种抽象的概念，并不真实存在，它描述的是**一组规则或规范**，通过这组规范定义了程序中各个变量（包括实例字段，静态字段和构成数组对象的元素）的访问方式。**JVM运行程序的实体是线程**，而每个线程创建时JVM都会为其创建一个**工作内存(有些地方称为栈空间)**，用于存储线程私有的数据。

而Java内存模型中规定所有变量都存储在主内存，**主内存是共享内存区域**，所有线程都可以访问，但线程对变量的操作(读取赋值等)必须在工作内存中进行，首先要将变量**从主内存拷贝的自己的工作内存空间**，然后对变量进行操作，操作完成后**再将变量写回主内存**，不能直接操作主内存中的变量，工作内存中存储着主内存中的变量副本拷贝，前面说过，工作内存是每个线程的私有数据区域，因此**不同的线程间无法访问对方的工作内存，线程间的通信(传值)必须通过主内存来完成。**

**概念图：**

![image.png](https://cdn.nlark.com/yuque/0/2021/png/705191/1623595159054-7fed6ad8-a511-4cdf-b53d-9cabd433d336.png)





## JMM与JVM内存的不同

### JMM内存

#### 主内存 

主要存储的是**Java实例对象(****堆内存****)**，所有线程创建的实例对象都存放在主内存中，不管该实例对象是成员变量还是方法中的本地变量(也称局部变量)，当然也包括了共享的类信息、常量、静态变量。由于是共享数据区域，多条线程对同一个变量进行访问可能会发生线程安全问题。

#### 工作内存

主要存储**当前方法的所有本地变量信息**(工作内存中存储着主内存中的变量副本拷贝)，每个线程**只能访问自己的工作内存**，即线程中的本地变量对其它线程是不可见的，就算是两个线程执行的是同一段代码，它们也会各自在自己的工作内存中创建属于当前线程的本地变量，也包括了字节码行号指示器、相关Native方法的信息(**也就是栈帧**)。注意由于工作内存是每个线程的私有数据，线程间无法相互访问工作内存，因此存储在工作内存的数据不存在线程安全问题。(**如上图所示**)



### JMM模型与硬件内存

毋庸置疑的是，多线程的执行最终都是归于硬件处理器去执行的。

对于硬件内存来说只有寄存器、缓存内存、主内存的概念，**并没有工作内存**(线程私有数据区域)和**主内存(堆内存)**之分，也就是说JMM对内存的划分对硬件内存并没有任何影响。

而JMM只是一种抽象的概念和规则，并没有实际存在，**不管是在工作内存还是主内存的数据，在计算机硬件的层次上都是存储在计算机主内存中的**，或者是可能存储在CPU缓存或者寄存器上。

总的来说，JMM和硬件内存是一个相互交叉的关系，是一种抽象概念与真实物理硬件的交叉。

**概念图：**

![image.png](https://cdn.nlark.com/yuque/0/2021/png/705191/1623648032551-b7dd0301-d908-4468-9594-5d1223c732bf.png?x-oss-process=image%2Fresize%2Cw_1500)





## JMM模型的重要性

#### 引言

在JVM中，运行程序的实体是线程，而每个线程创建时，JVM都会为其创建一个工作内存(栈空间)，用于存放现场私有的数据，而现场对于主内存的变量操作必须通过在其工作内存中操作完成。

大概过程是：

- 从主内存中拷贝需要的变量数据到线程的工作内存空间
- 在工作内存中对变量进行操作
- 操作完成后，再将变量写回主内存中(**同步**)

暂引出问题：

如果此时有两个甚至多个线程同时处理主内存中的某个变量，如x=1时，则可能诱发线程安全问题， 该如何解决呢？



## 数据同步的八大原子操作

#### 定义

关于主内存与工作内存之间的交互协议，即一个变量如何从主内存拷贝到工作内存。如何从工作内存同步到主内存中的实现细节。java内存模型定义了8种操作来完成，这8种操作每一种都是原子操作。

（1）lock(锁定)：作用于**主内存**的变量，把一个变量标记为一条线程独占状态 

（2）unlock(解锁)：作用于**主内存**的变量,把一个处于锁定状态的变量释放出来,释放后的变量才能被其他线程锁定 

（3）read(读取)：作用于**主内存**的变量，把一个变量值从主内存传输到线程的工作内存中,以便随后的load动作使用 

（4）load(载入)：作用于**工作内存**的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中 

（5）use(使用)：作用于**工作内存**的变量，把工作内存中的一个变量值传递给执行引擎 

（6）assign(赋值)：作用于**工作内存**的变量，它把一个从执行引擎接收到的值赋给工作内存的变量 

（7）store(存储)：作用于**工作内存**的变量，把工作内存中的一个变量的值传送到主内存中,以便随后的write的操作 

（8）write(写入)：作用于**工作内存**的变量，它把store操作从工作内存中的一个变量的值传送到主内存的变量中 



大致过程：

![image.png](https://cdn.nlark.com/yuque/0/2021/png/705191/1623652732769-eaf9c804-0df7-46a1-922f-c432d69ceb64.png)

#### 规则

1、不允许read和load、store和write操作之一单独出现（即不允许一个变量从主存读取了但是工作内存不接受，或者从工作内存发起会写了但是主存不接受的情况），以上两个操作必须按顺序执行，但没有保证必须连续执行，也就是说，read与load之间、store与write之间是可插入其他指令的。
2、不允许一个线程**丢弃它的最近的assign操作**，即变量在工作内存中改变了之后**必须把该变化同步回主内存**。
3、不允许一个线程无原因地（没有发生过任何assign操作）把数据从线程的工作内存同步回主内存中。
4、一个**新的变量只能从主内存中“诞生”**，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量，换句话说就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。
5、一个变量在同一个时刻只允许一条线程对其执行lock操作，但lock操作可以被同一个条线程重复执行多次，多次执行lock后，只有执行相同次数的unlock操作，变量才会被解锁。
6、如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行load或assign操作初始化变量的值。
7、如果一个变量实现没有被lock操作锁定，则不允许对它执行unlock操作，也不允许去unlock一个被其他线程锁定的变量。
8、对一个变量执行unlock操作之前，必须先把此变量同步回主内存（执行store和write操作）。





## 并发三大特性

#### 原子性

定义：即一个操作或者多个操作，要么全部执行并且执行的过程**不会被任何因素打断**，要么就都不执行。

注：Java里，对基本数据类型的变量的读取和赋值操作是原子性操作有点要注意的是，对 于32位系统的来说，long类型数据和double类型数据(对于基本数据类型，byte,short,int,float,boolean,char读写是原子操作)，它们的读写并非原子性的，因为对于32位虚拟机来说，每次原子读写是32位的，而long和double则是64位的存储单元。可能会出现不同现场读取数值不同的情况，如一个线程读的前32位，另一个线程读的后32位。

#### 可见性

定义：当一个线程**修改了某个共享变量**的值，**其他线程是否能够马上得知这个修改的值**。对于串行程序来说，可见性是不存在的，因为我们在任何一个操作中修改了某个变量的值，后续的操作中都能读取这个变量值，并且是修改过的新值。 

**引出**：从JMM模型中可以看出，当多线程操作时可能会出现问题，当某一个线程修改了某个共享变量时，另一个线程也在修改的话，哪一个线程最先修改完，且怎么通知另一个正在修改的线程是一个问题？因为数据同步回主内存是有一定的延迟的。以及指令重排、编译器优化等也可能导致可见性问题。

#### 有序性

定义：指对于单线程的执行代码，我们总是认为代码的执行是按顺序依次执行的，这样的理解并没有毛病，毕竟对于单线程而言确实如此，但对于多线程环境，则可能出现乱序现象，因为程序编译成机器码指令后可能会出现指令重排现象，重排后的指令与原指令的顺序未必一致。

### JMM与三大特性

#### 实现原子性

JVM自身提供的对基本数据那些读写操作可保证原子性。synchronized和Lock(保证任一时刻只有一个线程可访问某一处代码)可实现原子性。

#### 实现可见性

volatile关键字可实现可见性（类似通知功能，通知别的线程）。synchronized和Lock也可保证可见性(原子性基础上，释放锁之前会刷新值到内存中)。

#### 实现有序性

volatile关键字可保证一定的有序性(内存屏障、happens-before)。synchronized和Lock也可保证有序性(线程加锁，相当于顺序执行)



### 指令的重排序

java语言规范规定JVM线程内部维持顺序化语义。**即只要程序的最终结果与它顺序化情况的结果相等**，那么指令的执行顺序可以与代码顺序不一致，此过程叫指令的重排序。

指令重排序的意义是什么？JVM能根据处理器特性（CPU多级缓存系统、多核处理器等）适当的对机器指令进行重排序，使机器指令能更符合CPU的执行特性，最大限度的发挥机器性能以提高运行效率。

 



重排序分三种类型（排序即顺序）：

  1.**编译器优化的重排序**

编译器在不改变单线程程序语义的前提下（代码中不包含synchronized关键字），可以重新安排语句的执行顺序。

  2.**指令级并行的重排序**

现代处理器采用了指令级并行技术（Instruction-Level Parallelism， ILP）来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。

  3.**内存系统的重排序**
由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是在乱序执行



### as-if-serial原则

as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。

为了遵守as-if-serial语义，编译器和处理器不会对**存在数据依赖关系**的操作做重排序，因为这种重排序会**改变执行结果**。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 

如： a=1；b=a ； 先给a赋值为1(a初始为0)，假设重排了变成b=a ；a=1；那b的值不是一致的，则不会重排。



### happens-before原则

cpu的运行极快，而读取主存对于cpu而言有点慢了，在读取主存的过程中cpu一直闲着（也没数据可以运行），这对资源来说造成极大的浪费。所以慢慢的cpu演变成了多级cache结构，cpu在读cache的速度比读内存快了n倍。当线程在执行时，会保存临界资源的副本到私有work memory中，这个memory在cache中，修改这个临界资源会更新work memory但并不一定立刻刷到主存中，那么什么时候应该刷到主存中呢？什么时候和其他副本同步？而且编译器为了提高指令执行效率，是可以对指令重排序的，重排序后指令的执行顺序不一样，有可能线程2读取某个变量时，线程1还未进行写入操作。这就是线程可见性的来源，解决办法就是**happens-before规则**。



#### 定义

- 如果操作1 happens-before 操作2，那么操作1的执行结果将对操作2可见，而且操作1的执行顺序排在操作2之前。
- 两个操作之间存在happens-before关系，并不意味着一定要按照happens-before原则制定的顺序来执行。如果重排序之后的执行结果与按照happens-before关系来执行的结果一致，那么这种重排序并不非法。



#### 八大原则

- 程序顺序原则：在一个线程内必须保证语义的串行行，即按照代码书写的顺序执行语句。
- 锁定规则：一个解锁(unlock)操作先行发生于后面对同一个锁的加锁(lock)操作。即无论在单线程还是多线程中，**同一个锁**如果处于被锁定状态，那么必须先对锁进行解锁，后面才能继续执行加锁操作。

- volatile规则：对于一个被volatile修饰的变量，写操作先行于读操作。这就可以保证变量的可见性，当volatile变量被线程访问时，都需要从主内存中读取；当某个线程已经修改了volatile变量时，会强行将最新的值刷新到主内存，同时某些线程读取了之前的值会被通知作废，别的线程总是能读取到最新的值。
- 传递规则：操作A先行与操作B，操作B先行与操作C，那么操作A先行与操作C。

- 线程启动规则：Thread对象的start()方法先行发生于此线程的每一个动作。如果线程A在线程B start之前修改了主内存中的变量C(共享的)，那么线程B start后，也可以看到修改后的变量C值。
- 线程中断规则：对线程interrupt()方法的调用先行发生于被中断线程的代码检测到中断事件的发生。

- 线程终止规则：线程中所以的操作都先行发生于线程的终止检测，通过Thread.join()方法(等待当前执行的线程终止)、Thread.isAlive()的返回值手段检测到线程已经终止执行。
- 对象终结规则：一个对象的初始化完成先行于它的finalize(()方法的开始。



# volatile

## 作用原理

volatile是Java虚拟机提供的轻量级的同步机制。

两个作用 

- **可见性**：保证被volatile修饰的共享变量对所有线程总数可见的，也就是当一个线程修改了一个被volatile修饰共享变量的值，新值总是可以被其他线程立即得知。 
- **有序性**：禁止指令重排序优化

**但无法保证原子性**。



## 作用实现

### 可见性

```
package com.gx.demo.bingfa;

public class VolatileTest {
    
    private volatile boolean changeFlag = false;

    public void save() {
        this.changeFlag = true;
        System.out.println("线程：" + Thread.currentThread().getName() + " 修改了主存中的共享变量changeFlag");
    }

    public void load() {
        while (!changeFlag) {
        }
        System.out.println("线程：" + Thread.currentThread().getName() + " 感知到了changeFlag变量的修改");
    }

    public static void main(String[] args) {
        VolatileTest sample = new VolatileTest();
        Thread threadA = new Thread(() -> {
            sample.save();
        },"threadA");
        Thread threadB = new Thread(()-> {
                sample.load();
            },"threadB");
        threadB.start();
        try {
            Thread.sleep(2000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        threadA.start();
    }
}
结果：
线程：threadA:修改共享变量changeFlag
线程：threadB 感知到了changeFlag变量的修改
```

### 有序性(禁止指令重排)

禁止指令重排优化指的是：避免多线程环境下程序出现乱序执行的现象。



#### 内存屏障

**概念**：

什么是内存屏障（Memory Barrier）？
内存屏障（memory barrier）是一个CPU指令。

它的作用有两个： 

a) **确保一些特定操作执行的顺序**；

b) **影响一些数据的可见性**(可能是某些指令执行后的结果，保证在内存中可见)。

编译器和CPU可以在保证输出结果一样的情况下对指令重排序，使性能得到优化。插入一个内存屏障，**相当于告诉CPU和编译器先于这个命令的必须先执行**，后于这个命令的必须后执行。内存屏障另一个作用是强制更新一次不同CPU的缓存。

例如，一个写屏障会把这个屏障前写入的数据刷新到缓存，这样任何试图读取该数据的线程将得到最新值，而不用考虑到底是被哪个cpu核心或者哪颗CPU执行的。



#### 硬件层的内存屏障

Intel硬件提供了一系列的内存屏障，主要有：

1. lfence，是一种Load Barrier 读屏障
2. sfence, 是一种Store Barrier 写屏障

1. mfence, 是一种全能型的屏障，具备ifence和sfence的能力
2. Lock前缀，Lock不是一种内存屏障，但是它能完成类似内存屏障的功能。Lock会对CPU总线和高速缓存加锁，可以理解为CPU指令级的一种锁。它后面可以跟ADD, ADC, AND, BTC, BTR, BTS, CMPXCHG, CMPXCH8B, DEC, INC, NEG, NOT, OR, SBB, SUB, XOR, XADD, and XCHG等指令。



#### JVM提供的四类内存屏障

Java内存屏障主要有Load和Store两类。 
对Load Barrier(**读**)来说，在读指令前插入读屏障，可以让高速缓存中的数据失效，重新从主内存加载数据 
对Store Barrier(**写**)来说，在写指令之后插入写屏障，能让写入缓存的最新数据写回到主内存



对于Load和Store，在实际使用中，又分为以下四种：

| 屏障类型   | 指令例子                   | 说明用途                                                     |
| ---------- | -------------------------- | ------------------------------------------------------------ |
| LoadLoad   | Load1,Loadload,Load2       | 确保Load1所要读入的数据能够在被Load2和**后续的load**指令访问前读入 |
| StoreStore | Store1，StoreStore，Store2 | 确保Store1的数据在Store2以及**后续Store指令**操作相关数据之前对其它处理器可见（例如向主存刷新数据） |
| LoadStore  | Load1; LoadStore; Store2   | 确保Load1的数据在Store2和**后续Store**指令被刷新之前读取。在等待Store指令可以越过loads指令的乱序处理器上需要使用LoadStore屏障。 |
| StoreLoad  | Store1; StoreLoad; Load2   | 确保Store1的数据在被Load2和**后续的Load**指令读取之前对其他处理器可见 |

#### volatile的有序性实现

JMM针对编译器制定的volatile重排序规则表。 

第一个操作  第二个操作：普通读写 	第二个操作：volatile读 					 第二个操作：volatile写 

普通读写 	 可以重排 							可以重排 												不可以重排 

volatile读    不可以重排 						不可以重排 											不可以重排 

volatile写    可以重排 							不可以重排				        					  不可以重排



### 原子性

volatile无法保证原子性。

```
static volatile int i = 0;
public static void caculate(){
	i++;
}
```

i++分为：先去读取i的值，然后再+1写入一个新的值，两个步骤完成，本身不具备原子性。

假如在第一步完成之后，第二步执行之前时，有线程在此时读取了i在内存中的值，那么这个线程会和开始那个线程相当于要对i执行一样的操作，i结果都是1。也就造成线程安全失败了。

解决办法：对执行方法添加synchronized，但是synchronized一样具备了可见性，可以不用volatile修饰了。



# 缓存一致性

​	计算机在运行程序时，每条指令都是在CPU中执行的，在执行过程中势必会涉及到数据的读写。我们知道程序运行的数据是存储在主存中，这时就会有一个问题，读写主存中的数据没有CPU中执行指令的速度快，如果任何的交互都需要与主存打交道则会大大影响效率，所以就有了**CPU高速缓存**。CPU高速缓存为某个CPU独有，只与在该CPU运行的线程有关。
​	有了CPU高速缓存虽然解决了效率问题，但是它会带来一个新的问题：**数据一致性**。**在程序运行中，会将运行所需要的数据复制一份到CPU高速缓存中，在进行运算时CPU不再与主存打交道，而是直接从高速缓存中读写数据，只有当运行结束后才会将数据刷新到主存中。**
解决缓存一致性方案有两种：

1. 通过在总线加LOCK#锁的方式
2. 通过缓存一致性协议



## CPU缓存一致性协议



# 同步器

## 概念及意义

在多进程(线程)中，多个进程(线程)读取共享资源时会存在**竞争条件。**计算机中通过设计**同步器**来协调进程(线程)之间执行顺序。**同步器**作用就类似一个安检人员，可以协调旅客按顺序通过。

在Java中，**同步器**可以理解为一个对象，它根据自身状态协调线程的执行顺序。

比如锁（Lock），信号量（Semaphore），屏障（CyclicBarrier），阻塞队列（Blocking Queue）。

这些同步器在功能设计上有所不同，但是内部实现上有共通的地方。

（同步器的设计一般包含几个方面：**状态变量设计（同步器内部状态）**，**访问条件设定**，**状态更新**，**等待方式**，**通知策略**。）



在多线程编程中呢，也一样会可能出现多个线程同时访问同一个共享、可变资源的情况，这种资源称之为**临界资源**。比如文件、变量、对象等。

但线程执行的这个过程是不可控制的，需要同步器，即同步机制来处理，以保证线程的安全访问。



## 解决办法

了解了同步器存在的意义，基本可以知道并发模式的安全问题基本都是用此解决的，即**采用序列化方式访问临界资源**。也就是说，保证在同一时刻下，只有一个线程可以访问临界资源，实现**同步的互斥**。



在Java里，提供了两种方式实现：Lock锁和Synchronized(**因为同步器的本质就是加锁实现**)



# synchronized

## 发展

- JDK1.6之前：

synchronized 是一个**重量级锁**，主要通过内部对象Monitor实现(反编译字节码可提现)，而Monitor锁又是依赖于底层操作系统的Mutex Lock(互斥锁，互斥量)，调用Pthread库实现的，而Pthread库是处于系统的内核空间中，JVM存在于用户空间中，**故此非常耗时**。

- JDK1.6之前出现问题后：

**AQS**(AbstractQueuedSynchronizer)出现了，由国外某大佬用JAVA实现的各种锁，**保证了锁的可重入性和公平性，最重要的是耗时问题的解决**。

- JDK包括1.6之后：

在Java被oracle收购以后，为了实现自己的并发框架，对synchronized做了升级和改良，引入了**偏向锁、轻量级锁**等，让锁有了升级机制，解决了耗时和公平性等问题。



## 加锁方式

- 对于普通同步方法，锁是当前实例对象。
- 对于静态同步方法，锁是当前类的Class对象。

- 对于同步方法块，锁是synchronized括号里配置的对象。



## synchronized原理

对象头、各个锁hashcode 的存放位置

synchronized是一种**对象锁**(锁的是对象而非引用)，锁的**粒度是对象**，用来实现临界资源的同步于互斥，也具有**可重入性**。

JVM是基于进入和退出Moniter对象来实现方法同步和代码块同步，但两者细节有所区别。代码块同步时使用monitorenter和monitorexit指令实现的，方法是通过添加标志ACC_SYNCHRONIZED实现的。

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1624283098595-d27364f4-10b5-46b6-8d55-73625b5e6014.png)

而synchronized正是基于JVM的**内置锁Monitor(监视器锁)**实现的。监视器锁的实现以来底层操作系统的Mutex Lock(互斥锁)实现，它是一个重量级锁，性能较低。



## synchronized实现

**Java对象头和monitor是实现synchronized的基础**。



### Java对象头

synchronized用的锁是存在Java对象头里的。

在HotSpot，对象在内存中存储的布局可以分为3个区域：

1对象头（Header）

2实例数据（Instance Date）

3对齐填充（Padding）

**对象头**由Mark Word(标记字段)和Klass pointer(类型指针)组成。

Mark Word：(32位占4字节，64位占8字节)用于存储对象自身的运行时数据，如哈希码(Hash Code)、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等。(**对象和数组稍有差别，数组长度占4字节**)。

Klass pointer：(开启指针压缩占4字节，不开启占8字节)指对象指向它的类元数据的指针，JVM通过这个指针来确定这个对象是哪个类的实例。

下图是内存布局大概分布：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1617776742667-43bdf030-6f55-4d31-917b-2351afc45884.png)

下图是32位虚拟机对象头各锁对应的信息；

对象头一般占有两个机器码（在32位虚拟机中，1个机器码等于4字节，也就是32bit）），它是实现轻量级锁和

偏向锁的关键。

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1617776860916-57982473-5348-4a77-a956-e02d2d836f0a.png)



### Monitor

什么是Monitor？我们可以把它理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。 

与一切皆对象一样，所有的Java对象是天生的Monitor，每一个Java对象都有成为Monitor的潜质，因为在Java 

的设计中 ，每一个Java对象自打娘胎里出来就带了一把看不见的锁，它叫做**内部锁或者Monitor锁**。 



Monitor 是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列 

表。每一个被锁住的对象都会和一个monitor关联（对象头的MarkWord中的LockWord指向monitor的起始地 

址），同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。

如：某个线程进入了monitor(初始为0)，进入数+1，重复进入+1，退出-1。为0代表没有线程进入。

其结构如下：

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1624284866274-1f18d8a6-477e-48de-af8a-1c1e9bb4fc35.png)

- Owner：初始时为NULL表示当前没有任何线程拥有该monitor record，当线程成功拥有该锁后保存线程唯一标识，当锁被释放时又设置为NULL； 
- EntryQ:关联一个系统互斥锁（semaphore），阻塞所有试图锁住monitor record失败的线程。 

- RcThis:表示blocked或waiting在该monitor record上的所有线程的个数。 
- Nest:用来实现重入锁的计数。 

- HashCode:保存从对象头拷贝过来的HashCode值（可能还包含GC age）。 
- Candidate:用来避免不必要的阻塞或等待线程唤醒，因为每一次只有一个线程能够成功拥有锁，如果每次前一个释放锁的线程唤醒所有正在阻塞或等待的线程，会引起不必要的上下文切换（从阻塞到就绪然后因为竞争锁失败又被阻塞）从而导致性能严重下降。Candidate只有两种可能的值：0表示没有需要唤醒的线程，1表示要唤醒一个继任线程来竞争锁。



## 锁膨胀升级过程

锁的状态总共有四种：无锁状态、偏向锁、轻量级锁和重量级锁。

随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁，但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级。

从JDK 1.6 中**默认是开启**偏向锁和轻量级锁的，可以通过-XX:-UseBiasedLocking来禁用偏向锁。



### 偏向锁

HotSpot作者经过研究发现，在大多数情况下，锁不仅不存在多线程竞争，而且总是由同一线程多次获得，因此为了**减少同一线程获取锁**(会涉及到一些CAS操作,耗时)的代价而引入偏向锁。

偏向锁的核心思想是：如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，**当这个线程再次请求锁时**，获取锁无需再做任何同步操作，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。所以，对于没有锁竞争的场合，偏向锁有很好的优化效果，毕竟极有可能连续多次是同一个线程申请相同的锁。

但是对于锁竞争比较激烈的场合，偏向锁就失效了，因为这样场合极有可能每次申请锁的线程都是不相同的，因此这种场合下不应该使用偏向锁，否则会得不偿失，需要注意的是，**偏向锁失败后，并不会立即膨胀为重量级锁**，而是先升级为轻量级锁。



### 轻量级锁

**偏向锁失败后，并不会立即膨胀为重量级锁**，而是先升级为轻量级锁。与之同时，Mark Word中锁结构也会变成轻量级锁的结构。适用场景：绝大部分的锁，在整个同步周期内都不存在竞争，即不太存在同一时间访问同一锁的场合，不然会导致膨胀为重量级锁。



### 自旋锁

轻量级锁失败后，虚拟机为了避免线程真实地在操作系统层面挂起，还会进行一项称为自旋锁的优化手段。这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。（JVM让锁自旋，就是做空循环，一般50-100看具体设置）如果到了时间还没获取锁，就会在操作系统层面挂起线程，膨胀升级为重量级锁。



### 适应性自旋锁 

JDK 1.6引入了更加聪明的自旋锁，即自适应自旋锁。

所谓自适应就意味着自旋的次数不再是固定的，它是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。它怎么做呢？线程如果自旋成功了，那么下次自旋的次数会更加多，因为虚拟机认为既然上次成功了，那么此次自旋也很有可能会再次成功，那么它就会允许自旋等待持续的次数更多。

反之，如果对于某个锁，很少有自旋能够成功的，那么在以后要或者这个锁的时候自旋的次数会减少甚至省略掉自旋过程，以免浪费处理器资源。



### 锁消除 

为了保证数据的完整性，我们在进行操作时需要对这部分操作进行同步控制，但是在有些情况下，JVM检 

测到不可能存在共享数据竞争，这是JVM会对这些同步锁进行锁消除。**锁消除的依据是逃逸分析的数据支** 

**持**。 

如果不存在竞争，为什么还需要加锁呢？所以锁消除可以节省毫无意义的请求锁的时间。变量是否逃逸， 

对于虚拟机来说需要使用数据流分析来确定，但是对于我们程序员来说这还不清楚么？我们会在明明知道 

不存在数据竞争的代码块前加上同步吗？但是有时候程序并不是我们所想的那样？我们虽然没有显示使用 

锁，但是我们在使用一些JDK的内置API时，如StringBuffffer、Vector、HashTable等，这个时候会存在隐 

形的加锁操作。比如StringBuffffer的append()方法，Vector的add()方法。（如果JVM检测到变量没有逃 

逸，则会将其锁消除掉） 



**锁消除**：前提是java必须运行在server模式（server模式会比client模式作更多的优化），**同时必须开启逃逸分析** 

-XX:+DoEscapeAnalysis 开启逃逸分析 

-XX:+EliminateLocks 表示开启锁消除。 



**逃逸分析** 

使用逃逸分析，编译器可以对代码做如下优化： 

一、同步省略。如果一个对象被发现只能从一个线程被访问到，那么对于这个对象的操作可以不考虑同步。 

二、将堆分配转化为栈分配。如果一个对象在子程序中被分配，要使指向该对象的指针永远不会逃逸，对象可能是栈分配的候选，而不是堆分配。

三、分离对象或标量替换。有的对象可能不需要作为一个连续的内存结构存在也可以被访问到，那么对象的部分（或全部）可以不存储在内存， 

而是存储在CPU寄存器中。 



### 锁粗化 

在使用同步锁的时候，需要让同步块的作用范围尽可能小—仅在共享数据的实际作用域中才进行同步，这 

样做的目的是为了使需要同步的操作数量尽可能缩小，如果存在锁竞争，那么等待锁的线程也能尽快拿到 

锁。 

**锁粗化概念**：就是将多个连续的加锁、解锁操作连接在一起，扩展成一个范围更大的锁。如：vector每次 

add的时候都需要加锁操作，JVM检测到对同一个对象（vector）连续加锁、解锁操作，会合并一个更大范 

围的加锁、解锁操作，即加锁解锁操作会移到for循环之外。

再如一个类有好几个synchronized(xxObj)方法，都是锁的同一个对象，编译器会将其代码块操作都放到一个锁下面。

### 升级过程图

参考：https://gorden5566.com/post/1019.html

![img](https://cdn.nlark.com/yuque/0/2021/png/705191/1624344028666-a76d5b2c-136c-4f49-a3c3-5d671185de07.png)



# Lock接口

锁是用来控制多个线程访问共享资源的方式，一般来说，一个锁能够防止多个线程同时访问共享资源（但是有些锁可以允许多个线程并发的访问共享资源，比如读写锁）。在Lock接口出现之前，Java程序依靠synchronized关键字实现锁功能，而Java5之后，并发包(java.concurrent.util)新增了Lock接口(及相关实现类)来实现锁功能。它提供了与synchronized类似的同步功能，只是是**显示的获取和释放锁**（而synchronized是**隐式的获取和释放锁）**。也拥有了更多的synchronized不具备的同步特性，如锁的获取与释放可以多个操作、可中断的获取锁、超时的获取锁等。

synchronized是**隐式的获取和释放锁**，简化了同步的管理，但扩展性比较差，因为它固化了锁的获取和释放，必须先获取再释放。

如：

```
Lock lock = new ReentrantLock();
lock.lock();
try{

}finally{
  lock.unlock();
}
```

| 特性               | 描述                                                         |
| ------------------ | ------------------------------------------------------------ |
| 尝试非阻塞地获取锁 | 当现场尝试获取锁时，如果这一时刻锁没有其他线程占有，则成功获取并持有锁 |
| 能被中断地获取锁   | 与synchronized不同的是，获取到锁的线程能够响应中断，当获取到锁的线程被中断时，中断异常会被抛出，且同时会释放锁lock.lockInterruptibly() |
| 能超时获取锁       | 在指定的截止时间之前获取锁，如果截止时间到了仍然无法获取锁，则返回falselock.tryLock(timeout,TimeUnit) |





# 队列同步器AQS

## 简介

队列同步器AbstractQueuedSynchronizer，是用来**构建锁或者其他同步组件的基础框架**，它使用了一个int成员变量表示同步状态，通过内置的FIFO队列来完成资源获取线程的排队工作，并发包(java.concurrent.util)的作者Doug Lea。比如等待队列、条件队列、锁的独占和共享等，都是基于AQS，其定义了一套多线程访问共享资源的同步器框架。



## AQS的设计理念

前面提到过：同步器的设计一般包含几个方面：**状态变量设计（同步器内部状态）**，**访问条件设定**，**状态更新**，**等待方式**，**通知策略**。用Java实现的队列同步器AQS也是如此。

同步器的设计是基于模版方法模式的，也就是说，使用者需要继承同步器并重写指定的方法，随后将同步器组合在自定义同步组件的实现中，并调用同步器提供的模版方法，而这些模版方法将会调用使用者重写的方法。

与synchronized对比而言，就Lock接口多了一些特性，如锁的获取与释放可以多个操作、可中断的获取锁、超时的获取锁等。AQS也包括这些特性，以及实现了锁的公平性、非公平性，当然还有可重入性。

```
Lock lock = new 锁；
lock.lock();//加锁
for(;;){
	if(CAS操作){//加锁成功就跳出
    	break；
    }
    //否则需要等待，用什么数据结构保存？
    //即公平性与非公平性
}


lock.unlock();//解锁
```



## ReentrantLock



ReentrantLock

```
ReentrantLock lock = new ReentrantLock();
lock.lock();
try{

}finally{
  lock.unlock();
}
```



## AQS的接口框架



AQS它内部维护**volatile int state**（代表共享资源的可用状态）和一个**FIFO线程等待队列**（多线程争用资源被阻塞时会进入此队列）。

state的三种访问方式：

- getState()：返回当前的同步状态值
- setState(int)：设置当前的同步状态值

- compareAndSetState(int expect, int update)：原子操作替换状态值



AQS定义了两种队列

- 同步等待队列(CLH队列)
- 条件等待队列



AQS定义两种资源共享方式：

- Exclusive（独占，只有一个线程能执行，如ReentrantLock）
- Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）。



不同的自定义同步器争用共享资源的方式也不同。自定义同步器在实现时只需要实现共享资源state的获取与释放方式即可，至于具体线程等待队列的维护（如获取资源失败入队/唤醒出队等），AQS已经在顶层实现好了。自定义同步器实现时主要实现以下几种方法：

- isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。
- tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。

- tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。
- tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。

- tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false。



## AQS的部分源码理解
